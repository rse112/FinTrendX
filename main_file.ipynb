{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  버전 설치\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dtw import *\n",
    "import pickle\n",
    "from pytrends.request import TrendReq\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "from pytrends.request import TrendReq\n",
    "import nest_asyncio\n",
    "#from models.naver.blog import process_and_save_df , main_blog\n",
    "from api_set import APIClient\n",
    "import utils\n",
    "import models.crawling.trend as trend \n",
    "from models.crawling.collect_keywords import collect_keywords\n",
    "from models.crawling.google_trend import collect_rising_keywords\n",
    "from models.naver.news import main_news \n",
    "from models.crawling.select_keyword import select_keyword, rising_keyword_analysis, monthly_rule\n",
    "from models.anaysis import execute_analysis , process_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. API설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API 설정\n",
    "BASE_URL = utils.get_secret(\"BASE_URL\")\n",
    "CUSTOMER_ID = utils.get_secret(\"CUSTOMER_ID\")\n",
    "API_KEY = utils.get_secret(\"API_KEY\")\n",
    "SECRET_KEY = utils.get_secret(\"SECRET_KEY\")\n",
    "URI = utils.get_secret(\"URI\")\n",
    "METHOD = utils.get_secret(\"METHOD\")\n",
    "# API 클라이언트 인스턴스 생성\n",
    "api_client = APIClient(BASE_URL, CUSTOMER_ID, API_KEY, SECRET_KEY,URI,METHOD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 연관검색어 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키 로드\n",
    "keywords_data = utils.load_keywords('main_keyword.json')\n",
    "\n",
    "# 오늘의 날짜 가져오기\n",
    "formatted_today, day = utils.get_today_date()\n",
    "\n",
    "\n",
    "utils.make_directory('./data')\n",
    "utils.make_directory('./data/rl_srch')\n",
    "utils.make_directory(f'./data/rl_srch/{day}')  # 키워드별 연관검색어 리스트 저장\n",
    "\n",
    "# 검색어 리스트와 결과 저장 경로 설정\n",
    "srch_keyword = ['keyword_final']  \n",
    "save_path = './data/rl_srch/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'240326'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main(srch_keyword, day):\n",
    "    # 오늘 날짜로 폴더 경로 생성\n",
    "    folder_path = './data/rl_srch/' + datetime.now().strftime('%y%m%d')\n",
    "    file_path = f\"{folder_path}/collected_keywords.csv\"\n",
    "    \n",
    "    # 폴더가 존재하는지 확인\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # 파일이 존재하는지 확인\n",
    "    if os.path.isfile(file_path):\n",
    "        # 파일이 존재하면, 데이터를 읽어옵니다.\n",
    "        collected_keywords_data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 파일이 없으면, collect_keywords 함수를 호출해서 데이터를 수집합니다.\n",
    "        collected_keywords_data = await collect_keywords(srch_keyword, day)\n",
    "        # 결과를 CSV로 저장\n",
    "        collected_keywords_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    return collected_keywords_data\n",
    "collected_keywords_data=asyncio.run(main(srch_keyword, day))\n",
    "\n",
    "collected_keywords_dat_copy=asyncio.run(main(srch_keyword, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 중복검색어컬럼 생성, 중복제거하고 각 키워드별로 50개씩 집계하는 로직 \n",
    "\n",
    "# 1. collected_keywords_data의 복사본 생성\n",
    "temp_df = collected_keywords_data.copy()\n",
    "\n",
    "# 2. 새로운 컬럼 '중복검색어' 추가 (초기값으로 빈 문자열 할당)\n",
    "temp_df['중복검색어'] = ''\n",
    "\n",
    "# 3. 연관키워드별로 해당하는 모든 검색어를 찾는 딕셔너리 생성\n",
    "keywords_dict = {}\n",
    "for index, row in temp_df.iterrows():\n",
    "    associated_keyword = row['연관키워드']\n",
    "    search_keyword = row['검색어']\n",
    "    if associated_keyword in keywords_dict:\n",
    "        # 이미 리스트에 있는 경우 중복을 피하기 위해 추가하지 않음\n",
    "        if search_keyword not in keywords_dict[associated_keyword]:\n",
    "            keywords_dict[associated_keyword].append(search_keyword)\n",
    "    else:\n",
    "        # 새로운 키워드인 경우 리스트 초기화\n",
    "        keywords_dict[associated_keyword] = [search_keyword]\n",
    "\n",
    "# 4. '중복검색어' 컬럼을 채워 넣음\n",
    "for index, row in temp_df.iterrows():\n",
    "    associated_keyword = row['연관키워드']\n",
    "    # 연관키워드에 해당하는 모든 검색어를 '중복검색어' 컬럼에 할당\n",
    "    temp_df.at[index, '중복검색어'] = ','.join(keywords_dict[associated_keyword])\n",
    "order_list =  [\"주식\", \"금리\", \"금융상품\", \"디지털자산\", \"부동산\", \"세금\", \"재테크\", \"돈버는법\", \"테마주\", \"특징주\", \"외국인순매수\", \"신규상장\", \"급등주\", \"공모주\",\n",
    "  \"배당주\", \"미국주식\", \"주가지수\", \"WTI\", \"금값\", \"채권금리\", \"달러환율\", \"미국금리\",\n",
    "  \"ETF\", \"중개형ISA\", \"적립식펀드\", \"개인연금\", \"퇴직연금\", \"ELS\", \"CMA통장\", \"CMA금리비교\", \"채권\", \"신탁\", \"RP\", \"암호화폐\", \"미술품\", \"조각투자\", \n",
    "  \"아파트청약\", \"리워드\", \"캐시백\", \"돈버는앱\", \"건강보험\", \"자동차보험\", \"의료비보험\", \"상속\", \"증여\"]\n",
    "groups = temp_df.groupby('검색어')\n",
    "df_list_test = []\n",
    "already_selected = []\n",
    "\n",
    "for key in order_list:\n",
    "    if key in groups.groups:  # order_list에 있는 검색어가 실제로 그룹에 존재하는지 확인\n",
    "        group = groups.get_group(key)\n",
    "\n",
    "        selected_rows = []  # 선택된 행을 저장할 리스트\n",
    "        for index, row in group.iterrows():\n",
    "            if row['연관키워드'] not in already_selected:\n",
    "                selected_rows.append(row)\n",
    "                already_selected.append(row['연관키워드'])\n",
    "            if len(selected_rows) >= 50:\n",
    "                break\n",
    "        # 선택된 행들을 데이터프레임으로 변환하고 리스트에 추가\n",
    "        filtered_group = pd.DataFrame(selected_rows)\n",
    "        df_list_test.append(filtered_group)\n",
    "\n",
    "# 최종 데이터프레임 생성\n",
    "collected_keywords_data = pd.concat(df_list_test, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>연관키워드</th>\n",
       "      <th>월간검색수_합계</th>\n",
       "      <th>검색어</th>\n",
       "      <th>중복검색어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>삼천당제약주가</td>\n",
       "      <td>66900.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       연관키워드  월간검색수_합계  검색어 중복검색어\n",
       "666  삼천당제약주가   66900.0  공모주   공모주"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_keywords_data[collected_keywords_data['연관키워드'] == '삼천당제약주가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_keywords_data.to_csv(\"dd.csv\", index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 중복검색어컬럼 생성, 중복제거하고 각 키워드별로 50개씩 집계하는 로직 \n",
    "\n",
    "# # 1. collected_keywords_data의 복사본 생성\n",
    "# temp_df = collected_keywords_data.copy()\n",
    "\n",
    "# # 2. 새로운 컬럼 '중복검색어' 추가 (초기값으로 빈 문자열 할당)\n",
    "# temp_df['중복검색어'] = ''\n",
    "\n",
    "# # 3. 연관키워드별로 해당하는 모든 검색어를 찾는 딕셔너리 생성\n",
    "# keywords_dict = {}\n",
    "# for index, row in temp_df.iterrows():\n",
    "#     associated_keyword = row['연관키워드']\n",
    "#     search_keyword = row['검색어']\n",
    "#     if associated_keyword in keywords_dict:\n",
    "#         # 이미 리스트에 있는 경우 중복을 피하기 위해 추가하지 않음\n",
    "#         if search_keyword not in keywords_dict[associated_keyword]:\n",
    "#             keywords_dict[associated_keyword].append(search_keyword)\n",
    "#     else:\n",
    "#         # 새로운 키워드인 경우 리스트 초기화\n",
    "#         keywords_dict[associated_keyword] = [search_keyword]\n",
    "\n",
    "# # 4. '중복검색어' 컬럼을 채워 넣음\n",
    "# for index, row in temp_df.iterrows():\n",
    "#     associated_keyword = row['연관키워드']\n",
    "#     # 연관키워드에 해당하는 모든 검색어를 '중복검색어' 컬럼에 할당\n",
    "#     temp_df.at[index, '중복검색어'] = ','.join(keywords_dict[associated_keyword])\n",
    "# df_list_test = []\n",
    "# already_selected = set()\n",
    "# for _, group in temp_df.groupby('검색어'):\n",
    "#     selected_rows = []  # Collect rows to append\n",
    "#     for index, row in group.iterrows():\n",
    "#         if row['연관키워드'] not in already_selected:\n",
    "#             selected_rows.append(row)\n",
    "#             already_selected.add(row['연관키워드'])\n",
    "#         if len(selected_rows) >= 50:\n",
    "#             break\n",
    "#     # Append all selected rows at once to improve performance\n",
    "#     filtered_group = pd.DataFrame(selected_rows)\n",
    "#     df_list_test.append(filtered_group)\n",
    "# collected_keywords_data = pd.concat(df_list_test, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_list = [group for _, group in collected_keywords_data.groupby('검색어')]\n",
    "# collected_keywords_data = utils.merge_and_mark_duplicates_limited(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_keywords_data= utils.add_client_info(collected_keywords_data)\n",
    "new_columns = ['일별급상승', '주별급상승', '월별급상승', '주별지속상승', '월별지속상승', '월별규칙성']\n",
    "\n",
    "for column in new_columns:\n",
    "    collected_keywords_data[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>연관키워드</th>\n",
       "      <th>월간검색수_합계</th>\n",
       "      <th>검색어</th>\n",
       "      <th>중복검색어</th>\n",
       "      <th>id</th>\n",
       "      <th>pw</th>\n",
       "      <th>일별급상승</th>\n",
       "      <th>주별급상승</th>\n",
       "      <th>월별급상승</th>\n",
       "      <th>주별지속상승</th>\n",
       "      <th>월별지속상승</th>\n",
       "      <th>월별규칙성</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>삼천당제약주가</td>\n",
       "      <td>66900.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       연관키워드  월간검색수_합계  검색어 중복검색어                    id          pw  일별급상승  \\\n",
       "666  삼천당제약주가   66900.0  공모주   공모주  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0   \n",
       "\n",
       "     주별급상승  월별급상승  주별지속상승  월별지속상승  월별규칙성  \n",
       "666      0      0       0       0      0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_keywords_data[collected_keywords_data['연관키워드'] == '삼천당제약주가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def groupped_df(name,collected_keywords_data):\n",
    "    grouped = collected_keywords_data.groupby(name)\n",
    "    df_list = [group for _, group in grouped]\n",
    "    return df_list\n",
    "df_list=groupped_df('id',collected_keywords_data)\n",
    "n=len(df_list)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>연관키워드</th>\n",
       "      <th>월간검색수_합계</th>\n",
       "      <th>검색어</th>\n",
       "      <th>중복검색어</th>\n",
       "      <th>id</th>\n",
       "      <th>pw</th>\n",
       "      <th>일별급상승</th>\n",
       "      <th>주별급상승</th>\n",
       "      <th>월별급상승</th>\n",
       "      <th>주별지속상승</th>\n",
       "      <th>월별지속상승</th>\n",
       "      <th>월별규칙성</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>엔켐주가</td>\n",
       "      <td>380500.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>씨씨에스주가</td>\n",
       "      <td>310200.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>한화에어로스페이스주가</td>\n",
       "      <td>215500.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>삼성생명주가</td>\n",
       "      <td>144010.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,주가지수,CMA금리비교</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>SK이노베이션주가</td>\n",
       "      <td>142150.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>KB금융</td>\n",
       "      <td>112300.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>덕성주가</td>\n",
       "      <td>103610.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,미국주식,채권</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>KB금융주가</td>\n",
       "      <td>96680.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,주가지수</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>현대중공업주가</td>\n",
       "      <td>93800.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,주가지수,달러환율</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>삼성엔지니어링주가</td>\n",
       "      <td>90110.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>한국조선해양주가</td>\n",
       "      <td>86070.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,미국금리</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>LIG넥스원주가</td>\n",
       "      <td>82930.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>기업은행주가</td>\n",
       "      <td>78000.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>이마트주가</td>\n",
       "      <td>74000.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,주가지수</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>현대미포조선주가</td>\n",
       "      <td>73840.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>이수화학주가</td>\n",
       "      <td>73730.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>삼천당제약주가</td>\n",
       "      <td>66900.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>휴림로봇주가</td>\n",
       "      <td>65820.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>삼성증권주가</td>\n",
       "      <td>64030.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>NAVER주가</td>\n",
       "      <td>60840.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주,CMA금리비교</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>퇴직금</td>\n",
       "      <td>59700.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,중개형ISA,개인연금,퇴직연금,ELS,CMA통장</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>JYP주가</td>\n",
       "      <td>59250.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>삼성주가</td>\n",
       "      <td>58790.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주,달러환율</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>후성주가</td>\n",
       "      <td>57020.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주,주가지수</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>천보주가</td>\n",
       "      <td>55780.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>팬오션주가</td>\n",
       "      <td>55090.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>한전기술주가</td>\n",
       "      <td>54250.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,주가지수,달러환율</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>LG생활건강주가</td>\n",
       "      <td>54210.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주,달러환율</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>신라젠주가</td>\n",
       "      <td>53880.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>삼성주식</td>\n",
       "      <td>52420.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>특징주,공모주,채권</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>한국가스공사주가</td>\n",
       "      <td>51370.0</td>\n",
       "      <td>공모주</td>\n",
       "      <td>공모주,미국금리</td>\n",
       "      <td>Q7by3wnmDkoHjalsDvd0</td>\n",
       "      <td>ZAqEHDTStN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           연관키워드  월간검색수_합계  검색어                           중복검색어  \\\n",
       "650         엔켐주가  380500.0  공모주                             공모주   \n",
       "651       씨씨에스주가  310200.0  공모주                             공모주   \n",
       "652  한화에어로스페이스주가  215500.0  공모주                             공모주   \n",
       "653       삼성생명주가  144010.0  공모주                공모주,주가지수,CMA금리비교   \n",
       "654    SK이노베이션주가  142150.0  공모주                             공모주   \n",
       "655         KB금융  112300.0  공모주                             공모주   \n",
       "656         덕성주가  103610.0  공모주                     공모주,미국주식,채권   \n",
       "657       KB금융주가   96680.0  공모주                        공모주,주가지수   \n",
       "658      현대중공업주가   93800.0  공모주                   공모주,주가지수,달러환율   \n",
       "659    삼성엔지니어링주가   90110.0  공모주                             공모주   \n",
       "660     한국조선해양주가   86070.0  공모주                        공모주,미국금리   \n",
       "661     LIG넥스원주가   82930.0  공모주                             공모주   \n",
       "662       기업은행주가   78000.0  공모주                             공모주   \n",
       "663        이마트주가   74000.0  공모주                        공모주,주가지수   \n",
       "664     현대미포조선주가   73840.0  공모주                             공모주   \n",
       "665       이수화학주가   73730.0  공모주                         특징주,공모주   \n",
       "666      삼천당제약주가   66900.0  공모주                             공모주   \n",
       "667       휴림로봇주가   65820.0  공모주                         특징주,공모주   \n",
       "668       삼성증권주가   64030.0  공모주                             공모주   \n",
       "669      NAVER주가   60840.0  공모주                 특징주,공모주,CMA금리비교   \n",
       "670          퇴직금   59700.0  공모주  공모주,중개형ISA,개인연금,퇴직연금,ELS,CMA통장   \n",
       "671        JYP주가   59250.0  공모주                             공모주   \n",
       "672         삼성주가   58790.0  공모주                    특징주,공모주,달러환율   \n",
       "673         후성주가   57020.0  공모주                    특징주,공모주,주가지수   \n",
       "674         천보주가   55780.0  공모주                         특징주,공모주   \n",
       "675        팬오션주가   55090.0  공모주                             공모주   \n",
       "676       한전기술주가   54250.0  공모주                   공모주,주가지수,달러환율   \n",
       "677     LG생활건강주가   54210.0  공모주                    특징주,공모주,달러환율   \n",
       "678        신라젠주가   53880.0  공모주                         특징주,공모주   \n",
       "679         삼성주식   52420.0  공모주                      특징주,공모주,채권   \n",
       "680     한국가스공사주가   51370.0  공모주                        공모주,미국금리   \n",
       "\n",
       "                       id          pw  일별급상승  주별급상승  월별급상승  주별지속상승  월별지속상승  \\\n",
       "650  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "651  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "652  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "653  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "654  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "655  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "656  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "657  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "658  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "659  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "660  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "661  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "662  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "663  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "664  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "665  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "666  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "667  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "668  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "669  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "670  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "671  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "672  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "673  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "674  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "675  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "676  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "677  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "678  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "679  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "680  Q7by3wnmDkoHjalsDvd0  ZAqEHDTStN      0      0      0       0       0   \n",
       "\n",
       "     월별규칙성  \n",
       "650      0  \n",
       "651      0  \n",
       "652      0  \n",
       "653      0  \n",
       "654      0  \n",
       "655      0  \n",
       "656      0  \n",
       "657      0  \n",
       "658      0  \n",
       "659      0  \n",
       "660      0  \n",
       "661      0  \n",
       "662      0  \n",
       "663      0  \n",
       "664      0  \n",
       "665      0  \n",
       "666      0  \n",
       "667      0  \n",
       "668      0  \n",
       "669      0  \n",
       "670      0  \n",
       "671      0  \n",
       "672      0  \n",
       "673      0  \n",
       "674      0  \n",
       "675      0  \n",
       "676      0  \n",
       "677      0  \n",
       "678      0  \n",
       "679      0  \n",
       "680      0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[2].loc[650:680]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터를 로드하거나 크롤링하여 반환하는 비동기 함수\n",
    "async def load_or_crawl_data(df_list, clients):\n",
    "    today_date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory = f\"./data/trend_data/{today_date_str}\"\n",
    "    save_path = f\"{directory}/data_{today_date_str}.pkl\"\n",
    "    \n",
    "    # 파일이 존재하면 데이터 로드\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    else:\n",
    "        # 파일이 없으면 비동기 크롤링 시작\n",
    "        results = await run_all(df_list, clients)\n",
    "        # 결과 데이터 저장\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 비동기 크롤링 함수\n",
    "async def trend_main(df, clients):\n",
    "    params = {\n",
    "        \"search_keywords\": list(df['연관키워드']),\n",
    "        \"id\": df['id'].iloc[0],\n",
    "        \"pw\": df['pw'].iloc[0],\n",
    "        \"api_url\": \"https://openapi.naver.com/v1/datalab/search\",\n",
    "        \"name\": '연관검색어'\n",
    "    }\n",
    "    api_url = \"https://openapi.naver.com/v1/datalab/search\"\n",
    "    \n",
    "    # trend_maincode 함수 실행\n",
    "    results = await trend.trend_maincode(params, clients, api_url)\n",
    "    return results\n",
    "\n",
    "async def run_all(df_list, clients):\n",
    "    tasks = [trend_main(df, clients) for df in df_list]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "clients = utils.get_secret(\"clients\")  # clients 정보를 로드\n",
    "\n",
    "# 이벤트 루프 실행 및 데이터 로드 또는 크롤링\n",
    "trend_main_data = asyncio.run(load_or_crawl_data(df_list, clients))\n",
    "results = trend_main_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직렬로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>건강보험료</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>1.48849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>1.46660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>1.56511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>0.79148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>0.98676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-20</th>\n",
       "      <td>1.07144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-21</th>\n",
       "      <td>1.01902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-22</th>\n",
       "      <td>1.18204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-23</th>\n",
       "      <td>0.49136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-24</th>\n",
       "      <td>0.41359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1461 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              건강보험료\n",
       "date               \n",
       "2020-03-25  1.48849\n",
       "2020-03-26  1.46660\n",
       "2020-03-27  1.56511\n",
       "2020-03-28  0.79148\n",
       "2020-03-29  0.98676\n",
       "...             ...\n",
       "2024-03-20  1.07144\n",
       "2024-03-21  1.01902\n",
       "2024-03-22  1.18204\n",
       "2024-03-23  0.49136\n",
       "2024-03-24  0.41359\n",
       "\n",
       "[1461 rows x 1 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일별 급상승 키워드 발견: 갭투자\n",
      "일별 급상승 키워드 발견: 채권투자방법\n",
      "일별 급상승 키워드 발견: 배당금계산기\n",
      "일별 급상승 키워드 발견: 국채투자\n",
      "일별 급상승 키워드 발견: 중고노트북매입\n",
      "일별 급상승 키워드 발견: 심문\n",
      "일별 급상승 키워드 발견: 현관액자\n",
      "일별 급상승 키워드 발견: 신혼부부특별공급\n",
      "주별 급상승 키워드 발견: 암보험비갱신형\n",
      "주별 급상승 키워드 발견: 종합보험\n",
      "주별 급상승 키워드 발견: 비갱신암보험\n",
      "주별 급상승 키워드 발견: 수술비보험\n",
      "주별 급상승 키워드 발견: 보험비교사이트\n",
      "주별 급상승 키워드 발견: CI보험\n",
      "주별 급상승 키워드 발견: 자동차보험료계산\n",
      "주별 급상승 키워드 발견: 법인자동차보험\n",
      "주별 급상승 키워드 발견: 유병자실비보험\n",
      "주별 급상승 키워드 발견: 4세대실비보험\n",
      "주별 급상승 키워드 발견: 입원보험\n",
      "주별 급상승 키워드 발견: 유아보험\n",
      "주별 급상승 키워드 발견: 상속전문변호사\n",
      "주별 급상승 키워드 발견: 상속포기신고서\n",
      "주별 급상승 키워드 발견: 상속세신고\n",
      "주별 급상승 키워드 발견: 성년후견인신청\n",
      "주별 급상승 키워드 발견: 상속한정승인\n",
      "주별 급상승 키워드 발견: 상속등기\n",
      "주별 급상승 키워드 발견: 개인연금\n",
      "주별 급상승 키워드 발견: 매도\n",
      "주별 급상승 키워드 발견: 해외주식\n",
      "주별 급상승 키워드 발견: 비상금대출\n",
      "주별 급상승 키워드 발견: 대부대출\n",
      "주별 급상승 키워드 발견: 개인돈대출\n",
      "주별 급상승 키워드 발견: 대출상담\n",
      "주별 급상승 키워드 발견: 주식사이트\n",
      "주별 급상승 키워드 발견: DEX거래소\n",
      "주별 급상승 키워드 발견: 조세특례제한법\n",
      "주별 급상승 키워드 발견: 투자자산운용사\n",
      "주별 급상승 키워드 발견: 주식투자\n",
      "주별 급상승 키워드 발견: 프랜차이즈박람회\n",
      "주별 급상승 키워드 발견: 프랜차이즈창업\n",
      "주별 급상승 키워드 발견: SPYETF\n",
      "주별 급상승 키워드 발견: 해외주식추천\n",
      "주별 급상승 키워드 발견: 개인연금추천\n",
      "주별 급상승 키워드 발견: 삼성전자관련주\n",
      "주별 급상승 키워드 발견: 퇴직연금ETF\n",
      "주별 급상승 키워드 발견: 리딩방\n",
      "주별 급상승 키워드 발견: 오늘의주식시황\n",
      "주별 급상승 키워드 발견: 주식강의\n",
      "주별 급상승 키워드 발견: ETF투자\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword_df_group \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m keyword_df \u001b[38;5;129;01min\u001b[39;00m keyword_df_group:\n\u001b[1;32m---> 20\u001b[0m         selected_tmp, selected_graph, selected_info \u001b[38;5;241m=\u001b[39m \u001b[43mselect_keyword\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoday_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m selected_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;66;03m# 데이터프레임의 열 이름을 출력합니다.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m             selected_graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfoData\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m selected_info\n",
      "File \u001b[1;32mc:\\Users\\chohy\\OneDrive\\문서\\GitHub\\realdata\\models\\crawling\\select_keyword.py:338\u001b[0m, in \u001b[0;36mselect_keyword\u001b[1;34m(table, today, mode)\u001b[0m\n\u001b[0;32m    334\u001b[0m result_tmp, result_tmp_gph, table_graph \u001b[38;5;241m=\u001b[39m prepare_data(table, today, mode)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# print(\"result_tmp\",result_tmp)\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# print(\"result_tmp_gph\",result_tmp_gph)\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# print(\"table_graph\",table_graph)\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     dateLimit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m350\u001b[39m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweekly\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\chohy\\OneDrive\\문서\\GitHub\\realdata\\models\\crawling\\select_keyword.py:321\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(table, today, mode, days, year, years)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     result_tmp, result_tmp_gph \u001b[38;5;241m=\u001b[39m preprocess_data(\n\u001b[0;32m    318\u001b[0m         end_time,\n\u001b[0;32m    319\u001b[0m         table_tmp,\n\u001b[0;32m    320\u001b[0m         table_graph,\n\u001b[1;32m--> 321\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    322\u001b[0m         missing_data\u001b[38;5;241m=\u001b[39mmissing_data,\n\u001b[0;32m    323\u001b[0m         period\u001b[38;5;241m=\u001b[39mperiod,\n\u001b[0;32m    324\u001b[0m         Gap\u001b[38;5;241m=\u001b[39mGap,\n\u001b[0;32m    325\u001b[0m     )\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_tmp, result_tmp_gph, table_graph\n",
      "File \u001b[1;32mc:\\Users\\chohy\\OneDrive\\문서\\GitHub\\realdata\\models\\crawling\\select_keyword.py:230\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(end_time, table_tmp, table_graph, mode, missing_data, period, Gap)\u001b[0m\n\u001b[0;32m    228\u001b[0m tmp_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tmp_list)\u001b[38;5;241m.\u001b[39miloc[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# 그래프 작성용 데이터 집계\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m tmp_list_graph \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;241m0\u001b[39m, period[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m ):  \u001b[38;5;66;03m# 주의: 여기서 period는 그래프용 데이터 집계에 맞게 조정할 수 있습니다.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\chohy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:840\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    839\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 840\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    848\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    849\u001b[0m         arrays,\n\u001b[0;32m    850\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    853\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    854\u001b[0m     )\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chohy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chohy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:839\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m--> 839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_list_of_series_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;66;03m# last ditch effort\u001b[39;00m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\chohy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:885\u001b[0m, in \u001b[0;36m_list_of_series_to_arrays\u001b[1;34m(data, columns)\u001b[0m\n\u001b[0;32m    882\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m indexer_cache[\u001b[38;5;28mid\u001b[39m(index)] \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mget_indexer(columns)\n\u001b[0;32m    884\u001b[0m     values \u001b[38;5;241m=\u001b[39m extract_array(s, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 885\u001b[0m     aligned_values\u001b[38;5;241m.\u001b[39mappend(\u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    887\u001b[0m content \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(aligned_values)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Users\\chohy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chohy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "select_periods = [ 'daily','weekly','month']\n",
    "rising_periods=['weekly', 'month']\n",
    "\n",
    "formatted_today, today_date = utils.get_today_date()\n",
    "month_rule_list=[]\n",
    "select_list=[[],[],[]]\n",
    "\n",
    "rising_list=[[],[]]\n",
    "rising_month_list=[]\n",
    "\n",
    "rising_analysis_periods = ['weekly', 'month']\n",
    "\n",
    "\n",
    "i=0\n",
    "# # 일별, 주별, 월별 키워드 선택 실행\n",
    "for period in select_periods:\n",
    "    for keyword_df_group in results:\n",
    "        for keyword_df in keyword_df_group:\n",
    "            selected_tmp, selected_graph, selected_info = select_keyword(keyword_df, today_date, period)\n",
    "            if selected_graph is not None:\n",
    "                # 데이터프레임의 열 이름을 출력합니다.\n",
    "                selected_graph['InfoData'] = selected_info\n",
    "                select_list[i].append(selected_graph)\n",
    "            else:\n",
    "                pass\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for keyword_group in results:\n",
    "    # 키워드 그룹의 각 키워드 데이터프레임에 대해 순회합니다.\n",
    "    for keyword_data in keyword_group:\n",
    "        # 월별 규칙을 적용하여 결과를 가져옵니다.\n",
    "        monthly_data, monthly_chart, similarity_rate, rising_months = monthly_rule(keyword_data, today_date, 'month')\n",
    "        \n",
    "        if monthly_data is not None:\n",
    "            # 결과 데이터프레임의 열 이름을 가져옵니다.\n",
    "            column_names = monthly_data.columns\n",
    "            rising_month_list.append([rising_months,column_names[0]])\n",
    "            # 결과 데이터프레임에서 값 리스트를 추출합니다.\n",
    "            data_values_list = monthly_data[column_names].values\n",
    "            # 월별 차트에 데이터 값을 추가합니다.\n",
    "            monthly_chart['Indicator'] = data_values_list\n",
    "            monthly_chart['InfoData'] = similarity_rate\n",
    "            # 상승 월 정보를 추가합니다. 상승 월이 없는 경우 0으로 설정합니다.\n",
    "            monthly_chart['RisingMonth'] = 0\n",
    "\n",
    "            for i, month in enumerate(rising_months):\n",
    "                if i < len(monthly_chart['RisingMonth']):\n",
    "                   monthly_chart.loc[i, 'RisingMonth'] = month\n",
    "                else:\n",
    "                    break  # rising_months의 길이가 monthly_chart의 길이보다 길 경우, 나머지는 무시합니다.\n",
    "\n",
    "            \n",
    "            # 최종 결과 리스트에 수정된 월별 차트를 추가합니다.\n",
    "            month_rule_list.append(monthly_chart)\n",
    "\n",
    "            \n",
    "\n",
    "i = 0\n",
    "for period in rising_analysis_periods:\n",
    "    for keyword_df_group in results:\n",
    "        for keyword_df in keyword_df_group:\n",
    "            rising_tmp, rising_graph, rising_info = rising_keyword_analysis(keyword_df, today_date, period)\n",
    "            if rising_tmp is not None:\n",
    "                column_names=rising_tmp.columns\n",
    "                rising_graph['Indicator'] = 0\n",
    "                rising_graph['InfoData'] = rising_info\n",
    "\n",
    "                rising_list[i].append(rising_graph)\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "# # 월별, 주별, 일별 키워드 분석 실행\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Analysis completed in {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전역 변수로 리스트 초기화\n",
    "month_rule_list_a = []\n",
    "rising_list_a = [[], []]  # 주별 상승, 월별 상승\n",
    "select_list_a = [[], [], []]  # 일별 선택, 주별 선택, 월별 선택\n",
    "execute_analysis(results,month_rule_list_a,rising_list_a,select_list_a)\n",
    "\n",
    "\n",
    "\n",
    "select_list=[[],[],[]]\n",
    "\n",
    "rising_list=[[],[]]\n",
    "rising_month_list=[]\n",
    "\n",
    "\n",
    "# 각 리스트를 처리\n",
    "select_list[0] = process_results(select_list_a[0])\n",
    "select_list[1] = process_results(select_list_a[1])\n",
    "select_list[2] = process_results(select_list_a[2])\n",
    "\n",
    "rising_list[0] = process_results(rising_list_a[0])\n",
    "rising_list[1] = process_results(rising_list_a[1])\n",
    "\n",
    "# # month_rule_list_a를 처리하면서 추가 데이터 처리를 포함\n",
    "# for result in month_rule_list_a:\n",
    "#     if not all(value is None for value in result) and result[0] is not None:\n",
    "#         column_names = result[0].columns\n",
    "#         data_values_list = result[0][column_names].values\n",
    "#         additional_data = {\n",
    "#             'Indicator': data_values_list,\n",
    "#             'RisingMonth': 0,\n",
    "#             '유형': '월별규칙성'  # 모든 결과에 대해 '유형'을 '월별규칙성'으로 설정\n",
    "#         }\n",
    "#         month_rule_list += process_results([result], additional_data=additional_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_114720\\3902865495.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 리스트와 유형을 매핑\n",
    "lists_and_types = [\n",
    "    (select_list[0], '일별급상승'),\n",
    "    (select_list[1], '주별급상승'),\n",
    "    (select_list[2], '월별급상승'),\n",
    "    (rising_list[0], '주별지속상승'),\n",
    "    (rising_list[1], '월별지속상승'),\n",
    "    (month_rule_list, '월별규칙성')\n",
    "]\n",
    "\n",
    "\n",
    "# 모든 리스트를 처리하고 하나의 데이터프레임으로 병합\n",
    "processed_dfs = [utils.process_and_concat(df_list, label) for df_list, label in lists_and_types]\n",
    "\n",
    "\n",
    "# 비어 있지 않은 DataFrame들만 병합\n",
    "graph_result = pd.concat([df for df in processed_dfs if not df.empty]).reset_index(drop=True)\n",
    "\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "# 불필요한 컬럼 삭제 및 '주간지속상승'을 '주별지속상승'으로 수정\n",
    "\n",
    "graph_result = graph_result.drop(columns=['Indicator'])\n",
    "graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n",
    "graph_result['RisingMonth'] = graph_result['RisingMonth'].replace({None: 0})\n",
    "graph_result['RisingMonth'] = graph_result['RisingMonth'].fillna(0)\n",
    "# # 정렬\n",
    "graph_result.sort_values(by=['연관검색어', '유형', '검색일자'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "# # 최종 결과 출력\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_result[graph_result['RisingMonth']!=0].to_csv(\"ddss.csv\", index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_and_lists = [\n",
    "    (\"일별 급상승\", select_list[0]),\n",
    "    (\"주별 급상승\", select_list[1]),\n",
    "    (\"주별 지속상승\", rising_list[0]),\n",
    "    (\"월별 급상승\", select_list[2]),\n",
    "    (\"월별 지속상승\", rising_list[1]),\n",
    "    (\"월별 규칙성\", month_rule_list),\n",
    "]\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # data_list가 리스트인지 확인\n",
    "    if not isinstance(data_list, list):\n",
    "        print(f\"{flag_name}: data_list가 리스트가 아닙니다.\")\n",
    "        continue\n",
    "    \n",
    "    # data_list 내의 각 요소가 DataFrame인지, '연관검색어' 컬럼이 있는지 확인\n",
    "    for idx, df in enumerate(data_list):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"{flag_name}: 인덱스 {idx}에 DataFrame이 아닌 요소가 있습니다.\")\n",
    "        elif \"연관검색어\" not in df.columns:\n",
    "            print(f\"{flag_name}: 인덱스 {idx}의 DataFrame에 '연관검색어' 컬럼이 없습니다.\")\n",
    "\n",
    "# utils.update_keywords_flag 함수를 호출하기 전에 각 data_list의 유효성 검사\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # 데이터 프레임으로 구성된 리스트만 유지\n",
    "    valid_data_list = [df for df in data_list if isinstance(df, pd.DataFrame) and \"연관검색어\" in df.columns]\n",
    "    \n",
    "    # 유효한 데이터 리스트만을 사용하여 키워드 플래그 업데이트\n",
    "    utils.update_keywords_flag(collected_keywords_data, valid_data_list, flag_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # process_data : 지정된 조건에 따라 데이터를 필터링하고, 추가 처리를 통해 최종 데이터프레임을 반환하는 함수.\n",
    "def safe_process_data(process_function, data, category1, category2, selection):\n",
    "    \"\"\"\n",
    "    process_function: 데이터 처리 함수 (예: utils.process_data)\n",
    "    data: 처리할 데이터프레임\n",
    "    category1, category2: 데이터 처리 함수에 전달될 카테고리 인자\n",
    "    selection: 데이터 처리 함수에 전달될 선택 리스트 또는 기타 인자\n",
    "    \n",
    "    반환값: 처리된 데이터프레임 또는 빈 데이터프레임\n",
    "    \"\"\"\n",
    "    if data is not None and not data.empty:\n",
    "        try:\n",
    "            return process_function(data, category1, category2, selection)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data: {e}\")\n",
    "            # 처리 중 오류가 발생한 경우 빈 데이터프레임 반환\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No data available.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "info_result_daily_select = safe_process_data(utils.process_data, collected_keywords_data, '일별 급상승', '일별 급상승', select_list[0])\n",
    "\n",
    "info_result_weekly_select = utils.process_data(collected_keywords_data, '주별 급상승', '주별 급상승', select_list[1])\n",
    "info_result_monthly_select = utils.process_data(collected_keywords_data, '월별 급상승', '월별 급상승', select_list[2]) \n",
    "\n",
    "info_result_weekly_continuous = utils.process_data(collected_keywords_data, '주별 지속상승', '주별 지속상승', rising_list[0])\n",
    "\n",
    "info_result_monthly_continuous = utils.process_data(collected_keywords_data, '월별 지속상승', '월별 지속상승', rising_list[1])\n",
    "\n",
    "info_result_monthly_pattern = utils.process_data(collected_keywords_data, '월별 규칙성', '월별 규칙성', month_rule_list)\n",
    "\n",
    "info_result_final = pd.concat([info_result_daily_select,info_result_weekly_select, info_result_monthly_select,\\\n",
    "                               info_result_weekly_continuous, info_result_monthly_continuous,\\\n",
    "                                  info_result_monthly_pattern]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구글/ 네이버 한꺼번에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 뉴스링크,제목 수집 (네이버)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def collect_google_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"google_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            rising_keywords_results = pickle.load(file)\n",
    "    else:\n",
    "        rising_keywords_results = await collect_rising_keywords(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(rising_keywords_results, file)\n",
    "    \n",
    "    return rising_keywords_results\n",
    "\n",
    "async def collect_news_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"news_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            news_data = pickle.load(file)\n",
    "    else:\n",
    "        news_data = await main_news(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(news_data, file)\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# 메인 비동기 실행 함수\n",
    "async def main(target_keywords):\n",
    "    google_keywords_results, news_keywords_results = await asyncio.gather(\n",
    "        collect_google_keywords(target_keywords),\n",
    "        collect_news_keywords(target_keywords)\n",
    "    )\n",
    "    \n",
    "    return google_keywords_results, news_keywords_results\n",
    "\n",
    "\n",
    "\n",
    "target_keywords = list(set(info_result_final['연관키워드']))\n",
    "rising_keywords_results,news_data=asyncio.run(main(target_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/target_keywords/240326/keyword_activity_rates.csv에 작업 결과가 저장될 것입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################\n",
    "#활동성 분석\n",
    "################################\n",
    "import subprocess\n",
    "today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "directory_path = f\"./data/target_keywords/{today_date}\"\n",
    "file_path = os.path.join(directory_path, \"target_keywords.txt\")\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    # 디렉토리가 존재하지 않는 경우, 디렉토리 생성\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# 파일이 존재하는지 확인\n",
    "if not os.path.exists(file_path):\n",
    "    # 키워드를 파일에 작성\n",
    "    with open(file_path, 'w') as file:\n",
    "        for keyword in target_keywords:\n",
    "            file.write(\"%s\\n\" % keyword)\n",
    "    result = f\"{file_path}에 키워드 저장됨\"\n",
    "else:\n",
    "    result = f\"{file_path} 파일이 이미 존재합니다. 작업을 건너뜁니다.\"\n",
    "\n",
    "result\n",
    "\n",
    "# 파일이 존재하지 않는 경우, blog_data_collector.py 스크립트를 실행\n",
    "file_path = f\"./data/target_keywords/{today_date}/keyword_activity_rates.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    command = f\"python  models/naver/blog.py\"\n",
    "    process = subprocess.run(command, shell=True, check=True)\n",
    "    result = f\"{file_path}에 작업 결과가 저장될 것입니다.\"\n",
    "else:\n",
    "    result = f\"{file_path} 파일이 이미 존재합니다. 작업을 건너뜁니다.\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# 뉴스링크,제목,연관검색어 데이터프레임 생성\n",
    "#######################################\n",
    "\n",
    "\n",
    "name_list = list(news_data.keys())  \n",
    "# DataFrame 초기화\n",
    "news_df = pd.DataFrame()\n",
    "\n",
    "# 모든 키워드에 대해 처리\n",
    "for keyword in name_list:\n",
    "    # 뉴스 항목이 있는 경우 데이터 추가\n",
    "    for news_item in news_data[keyword]:\n",
    "        news_row = [keyword, news_item[0], news_item[1]]  # 연관키워드, 뉴스제목, 뉴스링크\n",
    "        news_df = pd.concat([news_df, pd.DataFrame([news_row])], ignore_index=True)\n",
    "\n",
    "    # 뉴스 항목 수가 10개에 미치지 못하면 나머지를 빈 행으로 채움\n",
    "    for _ in range(10 - len(news_data[keyword])):\n",
    "        empty_row = [keyword, None, None]  # 연관키워드, 빈 뉴스제목, 빈 뉴스링크\n",
    "        news_df = pd.concat([news_df, pd.DataFrame([empty_row])], ignore_index=True)\n",
    "\n",
    "# 칼럼 이름 설정\n",
    "news_df.columns = ['연관검색어', '뉴스제목', '뉴스링크']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_activity_rates = pd.read_csv(f'{directory_path}/keyword_activity_rates.csv')\n",
    "keyword_activity_rates.columns = ['연관검색어', '활동성']\n",
    "\n",
    "# '활동성' 열의 데이터를 백분율 형태의 문자열로 변환\n",
    "keyword_activity_rates['활동성'] = keyword_activity_rates['활동성'].apply(lambda x: f\"{x}%\")\n",
    "# news_df와 keyword_activity_rates를 '연관검색어' 열을 기준으로 병합\n",
    "keyword_activity_rates = keyword_activity_rates.drop_duplicates(subset=['연관검색어'])\n",
    "merged_keyword_activity_rates = pd.merge(news_df, keyword_activity_rates, on='연관검색어', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "# 네이버 merge\n",
    "####\n",
    "collected_keywords_dat_copy.rename(columns={'연관키워드': '연관검색어'}, inplace=True)\n",
    "info_result_final.rename(columns={'연관키워드': '연관검색어'}, inplace=True)\n",
    "# collected_keywords_dat_copy에서 '연관키워드'와 '검색어'를 기준으로 중복 제거\n",
    "collected_keywords_dat_copy = collected_keywords_dat_copy.drop_duplicates(subset=['연관검색어'], keep='first')\n",
    "# 이제 merged_keyword_activity_rates와 결합\n",
    "final_merged_df = pd.merge(merged_keyword_activity_rates, collected_keywords_dat_copy[['연관검색어', '검색어']], on='연관검색어', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_114720\\1304442105.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_keywords_by_search = collected_keywords_dat_copy.groupby('검색어').apply(\n"
     ]
    }
   ],
   "source": [
    "final_merged_df_copy = final_merged_df.copy()\n",
    "\n",
    "# 구글검색어 컬럼을 초기화합니다.\n",
    "final_merged_df_copy['구글검색어'] = None\n",
    "\n",
    "# 이후의 모든 작업은 final_merged_df_copy에 대해 수행합니다.\n",
    "i = 0\n",
    "for keyword, queries in rising_keywords_results.items():\n",
    "    filled_queries = queries[:10] + [None] * (10 - len(queries[:10]))\n",
    "    for query in filled_queries:\n",
    "        if i < len(final_merged_df_copy):\n",
    "            final_merged_df_copy.at[i, '구글검색어'] = query\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "# final_merged_df의 '검색어' 컬럼에서 각 10번째 검색어를 추출합니다.\n",
    "keyword_list_per_10 = final_merged_df_copy['검색어'].tolist()[::10]\n",
    "\n",
    "\n",
    " \n",
    "# collected_keywords_dat_copy에서 각 검색어별 상위 10개 연관검색어를 가져옵니다.\n",
    "# 여기서는 각 검색어별로 가장 높은 월간검색수를 가진 상위 10개를 선정합니다.\n",
    "top_keywords_by_search = collected_keywords_dat_copy.groupby('검색어').apply(\n",
    "    lambda x: x.nlargest(10, '월간검색수_합계')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# 새로운 DataFrame을 초기화합니다. 이 DataFrame에는 각 검색어별 상위 10개 연관검색어가 포함됩니다.\n",
    "new_rows_for_final_df = []\n",
    "\n",
    "\n",
    "for keyword in keyword_list_per_10:\n",
    "    # 특정 키워드에 대한 상위 10개 연관 검색어 추출\n",
    "    top_queries_for_keyword = top_keywords_by_search[top_keywords_by_search['검색어'] == keyword].head(10)\n",
    "    \n",
    "    # 추출된 연관 검색어를 결과 리스트에 추가\n",
    "    num_rows_added = 0  # 추가된 연관 검색어의 수를 추적\n",
    "    for _, row in top_queries_for_keyword.iterrows():\n",
    "        new_rows_for_final_df.append(row['연관검색어'])\n",
    "        num_rows_added += 1\n",
    "    \n",
    "    # 10개 미만인 경우 나머지를 None으로 채우기\n",
    "    for _ in range(10 - num_rows_added):\n",
    "        new_rows_for_final_df.append(None)\n",
    "\n",
    "\n",
    "# new_rows_for_final_df의 길이를 확인하고 final_merged_df의 '네이버검색어' 컬럼에 값을 할당합니다.\n",
    "# 주의: new_rows_for_final_df의 길이가 final_merged_df의 행 수와 동일해야 합니다.\n",
    "# 만약 길이가 다르다면, 길이가 맞도록 조정이 필요합니다.\n",
    "if len(new_rows_for_final_df) == len(final_merged_df_copy):\n",
    "    final_merged_df_copy['네이버검색어'] = new_rows_for_final_df\n",
    "else:\n",
    "    print(\"경고: '네이버검색어' 데이터의 길이가 final_merged_df와 다릅니다. 데이터 확인이 필요합니다.\")\n",
    "\n",
    "# 최종 DataFrame 확인\n",
    "#final_merged_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형식 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#info_result_final = info_result_final.drop(columns=[\"일별 급상승\", \"주별 급상승\", \"주별 지속상승\", \"월별 급상승\", \"월별 지속상승\", \"월별 규칙성\"])\n",
    "\n",
    "final_merged_df_result = pd.merge(info_result_final, final_merged_df_copy, how='left', on='연관검색어')\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# '기준일자' 컬럼을 가장 앞에 추가\n",
    "final_merged_df_result.insert(0, '기준일자', today_date)\n",
    "# 컬럼명 변경: '중복검색어' -> '검색키워드', '월간검색수_합계' -> '검색량'\n",
    "\n",
    "final_merged_df_result.rename(columns={'중복검색어': '검색키워드', '월간검색수_합계': '검색량'}, inplace=True)\n",
    "\n",
    "final_merged_df_result = final_merged_df_result.drop(columns=[\"검색어\"])\n",
    "\n",
    "\n",
    "final_merged_df_result['상승월'] = None\n",
    "\n",
    "\n",
    "\n",
    "# 형식맞추기 위한 info_result_final 순서 정렬\n",
    "info_result_af_copy=pd.DataFrame()\n",
    "a = final_merged_df_result.query(\"`유형` == '일별 급상승'\")\n",
    "b = final_merged_df_result.query(\"`유형` == '주별 급상승' or `유형` == '주별 지속상승'\")\n",
    "c = final_merged_df_result.query(\"`유형` == '월별 급상승' or `유형` == '월별 지속상승' or `유형` == '월별 규칙성'\")\n",
    "a_sort=a.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "b_sort = b.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "c_sort = c.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "info_result_af_copy=pd.concat([a_sort,b_sort,c_sort])\n",
    "\n",
    "# 형식을 위한 이름 변경\n",
    "new_column_order = ['기준일자', '유형', '연관검색어', '검색키워드', '검색량', '지표', '뉴스제목', '뉴스링크', '활동성', '구글검색어', '네이버검색어', '상승월']\n",
    "info_result_af_copy_reordered = info_result_af_copy[new_column_order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유형 순서 정렬\n",
    "info_result_af_copy_reordered_modified = info_result_af_copy_reordered.copy()\n",
    "\n",
    "\n",
    "# 인덱스 재설정\n",
    "info_result_af_copy_reordered_modified.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sort_order = {\n",
    "    \"일별 급상승\": 1,\n",
    "    \"주별 급상승\": 2,\n",
    "    \"주별 지속상승\": 3,\n",
    "    \"월별 급상승\": 4,\n",
    "    \"월별 지속상승\": 5,\n",
    "    \"월별 규칙성\" : 6\n",
    "}\n",
    "\n",
    "# 유형 컬럼에 대한 정렬 순서를 적용하기 위해 임시 컬럼 추가\n",
    "info_result_af_copy_reordered_modified['sort_key'] = info_result_af_copy_reordered_modified['유형'].map(sort_order)\n",
    "\n",
    "# 임시 컬럼을 기준으로 정렬\n",
    "info_result_af_copy_reordered_modified = info_result_af_copy_reordered_modified.sort_values(by=['sort_key', '연관검색어'], ascending=[True, True])\n",
    "\n",
    "# 임시 컬럼 삭제\n",
    "info_result_af_copy_reordered_modified.drop('sort_key', axis=1, inplace=True)\n",
    "\n",
    "info_result_af_copy_reordered_modified.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pattern_df = info_result_af_copy_reordered_modified[info_result_af_copy_reordered_modified['유형'] == '월별 규칙성'].copy()\n",
    "df_without_monthly_pattern = info_result_af_copy_reordered_modified[info_result_af_copy_reordered_modified['유형'] != '월별 규칙성'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_result_af_copy_reordered_modified의 '상승월' 업데이트\n",
    "for index, row in info_result_af_copy_reordered_modified.iterrows():\n",
    "    # graph_result에서 일치하는 '연관검색어'와 '유형'이 '월별규칙성'인 행 찾기\n",
    "    matching_rows = graph_result[(graph_result['연관검색어'] == row['연관검색어']) & (graph_result['유형'] == '월별규칙성')]\n",
    "    \n",
    "    # 일치하는 행들의 'RisingMonth' 값들을 리스트로 가져오기\n",
    "    if not matching_rows.empty:\n",
    "        rising_months_list = matching_rows['RisingMonth'].tolist()\n",
    "        # '상승월' 열에 값 할당\n",
    "        info_result_af_copy_reordered_modified.at[index, '상승월'] = ', '.join(map(str, rising_months_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pattern_df = info_result_af_copy_reordered_modified[info_result_af_copy_reordered_modified['유형'] == '월별 규칙성'].copy()\n",
    "df_without_monthly_pattern = info_result_af_copy_reordered_modified[info_result_af_copy_reordered_modified['유형'] != '월별 규칙성'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pattern_dff = pd.DataFrame()\n",
    "dfs = []\n",
    "\n",
    "# 연관검색어별로 그룹화하여 처리\n",
    "for key, group in monthly_pattern_df.groupby('연관검색어'):\n",
    "    # 상승월 데이터 분리\n",
    "    rising_months = group['상승월'].iloc[0].split(',')[:10]\n",
    "    group = group.drop('상승월', axis=1)\n",
    "    group['상승월'] = rising_months\n",
    "    monthly_pattern_dff = pd.concat([monthly_pattern_dff, group])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_monthly_pattern['상승월']=None\n",
    "monthly_pattern_dff['상승월'] = monthly_pattern_dff['상승월'].apply(lambda x: None if x == 0 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_infodf=pd.concat([df_without_monthly_pattern,monthly_pattern_dff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형식에 맞춰서 띄어쓰기 변경\n",
    "# '유형' 컬럼의 값을 바꾸기 위한 딕셔너리 정의\n",
    "replace_values = {\n",
    "    '일별 급상승': '일별급상승',\n",
    "    '주별 급상승': '주별급상승',\n",
    "    '주별 지속상승': '주별지속상승',\n",
    "    '월별 급상승': '월별급상승',\n",
    "    '월별 지속상승': '월별지속상승',\n",
    "    '월별 규칙성': '월별규칙성'\n",
    "}\n",
    "\n",
    "# '유형' 컬럼 내의 값을 바꾸기\n",
    "graph_result['유형'] = graph_result['유형'].replace(replace_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_related_search_terms = list(graph_result[pd.isna(graph_result['검색량'])]['연관검색어'])\n",
    "unique_na_related_search_terms  = list(set(na_related_search_terms))\n",
    "\n",
    "filtered_graph_result = graph_result[~graph_result['연관검색어'].isin(unique_na_related_search_terms)]\n",
    "\n",
    "\n",
    "filtered_info_result_af_copy_reordered_modified = info_result_af_copy_reordered_modified[~info_result_af_copy_reordered_modified['연관검색어'].isin(unique_na_related_search_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_graph_result_updated = filtered_graph_result.iloc[:, :-2]\n",
    "filtered_graph_result_updated_a = filtered_graph_result_updated[filtered_graph_result_updated['유형'] == '일별급상승']\n",
    "\n",
    "# 올바른 조건을 사용하여 필터링\n",
    "filtered_graph_result_updated_b = filtered_graph_result_updated[\n",
    "    filtered_graph_result_updated['유형'].isin(['월별급상승', '월별지속상승', '월별규칙성'])]\n",
    "filtered_graph_result_updated_c = filtered_graph_result_updated[\n",
    "    filtered_graph_result_updated['유형'].isin(['주별급상승', '주별지속상승'])]\n",
    "sorted_filtered_graph_result_updated_b = filtered_graph_result_updated_b.sort_values(by=['연관검색어', '유형', '검색일자'])\n",
    "sorted_filtered_graph_result_updated_c = filtered_graph_result_updated_c.sort_values(by=['연관검색어', '유형', '검색일자'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([filtered_graph_result_updated_a, sorted_filtered_graph_result_updated_c, sorted_filtered_graph_result_updated_b], axis=0)\n",
    "combined_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전송용 결과 테이블 생성 함수\n",
    "\n",
    "def make_csv(table) :\n",
    "\n",
    "  # 컬럼 추출\n",
    "  col_a = ''\n",
    "  col_b = ''\n",
    "\n",
    "  for col in table.columns :\n",
    "    col_a = str(col) + '|||'\n",
    "    col_b = col_b + col_a\n",
    "  col_b\n",
    "\n",
    "\n",
    "  # 행 추출\n",
    "  row_list = []\n",
    "\n",
    "  for j in range(0, len(table)) :\n",
    "    tmp_a = ''\n",
    "    tmp_b = ''\n",
    "\n",
    "    for i in range(0, len(table.columns)) :\n",
    "      tmp_a = str(table.iloc[j,i]) + '|||'\n",
    "      tmp_b = tmp_b + tmp_a\n",
    "    row_list.append(tmp_b)\n",
    "\n",
    "  row_list.insert(0,col_b)\n",
    "  df = pd.DataFrame(row_list)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data = result_infodf.fillna(' ')\n",
    "info_data.reset_index(inplace = True, drop = True)\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data.to_csv(\"infotest.csv\",encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = make_csv(info_data)\n",
    "\n",
    "# 현재 날짜를 'yyMMdd' 형식으로 포맷팅\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n",
    "\n",
    "# 저장할 경로\n",
    "save_path = f'./data/result_out/{formatted_today}'\n",
    "\n",
    "# 해당 경로가 존재하지 않으면 생성\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# CSV 파일 저장\n",
    "result_csv.to_csv(f'{save_path}/info_{formatted_today}.csv', encoding='utf-8-sig', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_a = combined_df[combined_df['유형'] == '일별급상승']\n",
    "\n",
    "# 올바른 조건을 사용하여 필터링\n",
    "combined_df_b = combined_df[\n",
    "    combined_df['유형'].isin(['월별급상승', '월별지속상승', '월별규칙성'])]\n",
    "combined_df_c = combined_df[\n",
    "    combined_df['유형'].isin(['주별급상승', '주별지속상승'])]\n",
    "sorted_combined_df__b = combined_df_b.sort_values(by=['유형', '연관검색어', '검색일자'])\n",
    "sorted_combined_df__c = combined_df_c.sort_values(by=['유형', '연관검색어', '검색일자'])\n",
    "combined_df = pd.concat([combined_df_a, sorted_combined_df__c, sorted_combined_df__b], axis=0)\n",
    "combined_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_graph = make_csv(combined_df)\n",
    "\n",
    "\n",
    "# 현재 날짜를 'yyMMdd' 형식으로 포맷팅\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n",
    "\n",
    "# 저장할 경로\n",
    "save_path = f'./data/result_out/{formatted_today}'\n",
    "\n",
    "# 해당 경로가 존재하지 않으면 생성\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# CSV 파일 저장\n",
    "result_graph.to_csv(f'{save_path}/graph_{formatted_today}.csv', encoding='utf-8-sig', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
