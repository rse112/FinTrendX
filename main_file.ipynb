{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  버전 설치\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dtw import *\n",
    "import pickle\n",
    "from pytrends.request import TrendReq\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "from pytrends.request import TrendReq\n",
    "import nest_asyncio\n",
    "from models.naver.blog import blog_result_async,main_blog_analysis\n",
    "from api_set import APIClient\n",
    "import utils\n",
    "import models.crawling.trend as trend \n",
    "from models.crawling.collect_keywords import collect_keywords\n",
    "from models.crawling.google_trend import collect_rising_keywords\n",
    "from models.naver.news import main_news \n",
    "from models.crawling.select_keyword import select_keyword, rising_keyword_analysis, monthly_rule\n",
    "from models.anaysis import execute_analysis , process_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. API설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API 설정\n",
    "BASE_URL = utils.get_secret(\"BASE_URL\")\n",
    "CUSTOMER_ID = utils.get_secret(\"CUSTOMER_ID\")\n",
    "API_KEY = utils.get_secret(\"API_KEY\")\n",
    "SECRET_KEY = utils.get_secret(\"SECRET_KEY\")\n",
    "URI = utils.get_secret(\"URI\")\n",
    "METHOD = utils.get_secret(\"METHOD\")\n",
    "# API 클라이언트 인스턴스 생성\n",
    "api_client = APIClient(BASE_URL, CUSTOMER_ID, API_KEY, SECRET_KEY,URI,METHOD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 연관검색어 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키 로드\n",
    "keywords_data = utils.load_keywords('main_keyword.json')\n",
    "\n",
    "# 오늘의 날짜 가져오기\n",
    "formatted_today, day = utils.get_today_date()\n",
    "\n",
    "\n",
    "utils.make_directory('./data')\n",
    "utils.make_directory('./data/rl_srch')\n",
    "utils.make_directory(f'./data/rl_srch/{day}')  # 키워드별 연관검색어 리스트 저장\n",
    "\n",
    "# 검색어 리스트와 결과 저장 경로 설정\n",
    "srch_keyword = ['keyword_final']  \n",
    "save_path = './data/rl_srch/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main(srch_keyword, day):\n",
    "    # 오늘 날짜로 폴더 경로 생성\n",
    "    folder_path = './data/rl_srch/' + datetime.now().strftime('%y%m%d')\n",
    "    file_path = f\"{folder_path}/collected_keywords.csv\"\n",
    "    \n",
    "    # 폴더가 존재하는지 확인\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # 파일이 존재하는지 확인\n",
    "    if os.path.isfile(file_path):\n",
    "        # 파일이 존재하면, 데이터를 읽어옵니다.\n",
    "        collected_keywords_data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 파일이 없으면, collect_keywords 함수를 호출해서 데이터를 수집합니다.\n",
    "        collected_keywords_data = await collect_keywords(srch_keyword, day)\n",
    "        # 결과를 CSV로 저장\n",
    "        collected_keywords_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    return collected_keywords_data\n",
    "collected_keywords_data=asyncio.run(main(srch_keyword, day))\n",
    "\n",
    "collected_keywords_dat_copy=asyncio.run(main(srch_keyword, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# '검색어'별로 그룹화된 DataFrame을 리스트에 저장\n",
    "df_list = [group for _, group in collected_keywords_data.groupby('검색어')]\n",
    "collected_keywords_data = utils.merge_and_mark_duplicates_limited(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_keywords_data= utils.add_client_info(collected_keywords_data)\n",
    "new_columns = ['일별급상승', '주별급상승', '월별급상승', '주별지속상승', '월별지속상승', '월별규칙성']\n",
    "\n",
    "for column in new_columns:\n",
    "    collected_keywords_data[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupped_df(name,collected_keywords_data):\n",
    "    grouped = collected_keywords_data.groupby(name)\n",
    "    df_list = [group for _, group in grouped]\n",
    "    return df_list\n",
    "df_list=groupped_df('id',collected_keywords_data)\n",
    "n=len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터를 로드하거나 크롤링하여 반환하는 비동기 함수\n",
    "async def load_or_crawl_data(df_list, clients):\n",
    "    today_date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory = f\"./data/trend_data/{today_date_str}\"\n",
    "    save_path = f\"{directory}/data_{today_date_str}.pkl\"\n",
    "    \n",
    "    # 파일이 존재하면 데이터 로드\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    else:\n",
    "        # 파일이 없으면 비동기 크롤링 시작\n",
    "        results = await run_all(df_list, clients)\n",
    "        # 결과 데이터 저장\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 비동기 크롤링 함수\n",
    "async def trend_main(df, clients):\n",
    "    params = {\n",
    "        \"search_keywords\": list(df['연관키워드']),\n",
    "        \"id\": df['id'].iloc[0],\n",
    "        \"pw\": df['pw'].iloc[0],\n",
    "        \"api_url\": \"https://openapi.naver.com/v1/datalab/search\",\n",
    "        \"name\": '연관검색어'\n",
    "    }\n",
    "    api_url = \"https://openapi.naver.com/v1/datalab/search\"\n",
    "    \n",
    "    # trend_maincode 함수 실행\n",
    "    results = await trend.trend_maincode(params, clients, api_url)\n",
    "    return results\n",
    "\n",
    "async def run_all(df_list, clients):\n",
    "    tasks = [trend_main(df, clients) for df in df_list]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "clients = utils.get_secret(\"clients\")  # clients 정보를 로드\n",
    "\n",
    "# 이벤트 루프 실행 및 데이터 로드 또는 크롤링\n",
    "trend_main_data = asyncio.run(load_or_crawl_data(df_list, clients))\n",
    "results = trend_main_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start_time = time.time()\n",
    "# select_periods = ['daily', 'weekly', 'month']\n",
    "# rising_periods=['weekly', 'month']\n",
    "\n",
    "# formatted_today, today_date = utils.get_today_date()\n",
    "# month_rule_list=[]\n",
    "# select_list=[[],[],[]]\n",
    "\n",
    "# rising_list=[[],[]]\n",
    "# rising_month_list=[]\n",
    "# # 월별, 주별, 일별 키워드 분석 실행\n",
    "\n",
    "#     # 각 분석 기간에 대해 결과 집합을 순회합니다.\n",
    "# for keyword_group in results:\n",
    "#     # 키워드 그룹의 각 키워드 데이터프레임에 대해 순회합니다.\n",
    "#     for keyword_data in keyword_group:\n",
    "#         # 월별 규칙을 적용하여 결과를 가져옵니다.\n",
    "#         monthly_data, monthly_chart, similarity_rate, rising_months = monthly_rule(keyword_data, today_date, 'month')\n",
    "        \n",
    "#         if monthly_data is not None:\n",
    "#             # 결과 데이터프레임의 열 이름을 가져옵니다.\n",
    "#             column_names = monthly_data.columns\n",
    "#             rising_month_list.append([rising_months,column_names[0]])\n",
    "#             # 결과 데이터프레임에서 값 리스트를 추출합니다.\n",
    "#             data_values_list = monthly_data[column_names].values\n",
    "#             # 월별 차트에 데이터 값을 추가합니다.\n",
    "#             monthly_chart['Indicator'] = data_values_list\n",
    "#             monthly_chart['InfoData'] = similarity_rate\n",
    "#             # 상승 월 정보를 추가합니다. 상승 월이 없는 경우 0으로 설정합니다.\n",
    "#             monthly_chart['RisingMonth'] = 0\n",
    "            \n",
    "#             # 최종 결과 리스트에 수정된 월별 차트를 추가합니다.\n",
    "#             month_rule_list.append(monthly_chart)\n",
    "                \n",
    "# # 주별, 월별 상승 키워드 분석 실행\n",
    "# rising_analysis_periods = ['weekly', 'month']\n",
    "# i=0\n",
    "# for period in rising_analysis_periods:\n",
    "#     for keyword_df_group in results:\n",
    "#         for keyword_df in keyword_df_group:\n",
    "#             rising_tmp, rising_graph, rising_info = rising_keyword_analysis(keyword_df, today_date, period)\n",
    "#             if rising_tmp is not None:\n",
    "#                 column_names=rising_tmp.columns\n",
    "#                 data_values_list = rising_tmp[column_names].values\n",
    "#                 rising_graph['Indicator'] = data_values_list\n",
    "#                 rising_graph['InfoData'] = rising_info\n",
    "\n",
    "#                 rising_list[i].append(rising_graph)\n",
    "#     i=i+1\n",
    "\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# # 일별, 주별, 월별 키워드 선택 실행\n",
    "# for period in select_periods:\n",
    "#     for keyword_df_group in results:\n",
    "#         for keyword_df in keyword_df_group:\n",
    "#             selected_tmp, selected_graph, selected_info = select_keyword(keyword_df, today_date, period)\n",
    "#             if selected_graph is not None:\n",
    "#                 # 데이터프레임의 열 이름을 출력합니다.\n",
    "#                 selected_graph['InfoData'] = selected_info\n",
    "#                 select_list[i].append(selected_graph)\n",
    "#             else:\n",
    "#                 pass\n",
    "#     i += 1\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"Analysis completed in {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전역 변수로 리스트 초기화\n",
    "month_rule_list_a = []\n",
    "rising_list_a = [[], []]  # 주별 상승, 월별 상승\n",
    "select_list_a = [[], [], []]  # 일별 선택, 주별 선택, 월별 선택\n",
    "future = execute_analysis(results,month_rule_list_a,rising_list_a,select_list_a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_rule_list=[]\n",
    "select_list=[[],[],[]]\n",
    "\n",
    "rising_list=[[],[]]\n",
    "rising_month_list=[]\n",
    "\n",
    "\n",
    "# 각 리스트를 처리\n",
    "select_list[0] = process_results(select_list_a[0])\n",
    "select_list[1] = process_results(select_list_a[1])\n",
    "select_list[2] = process_results(select_list_a[2])\n",
    "\n",
    "rising_list[0] = process_results(rising_list_a[0])\n",
    "rising_list[1] = process_results(rising_list_a[1])\n",
    "\n",
    "# month_rule_list_a를 처리하면서 추가 데이터 처리를 포함\n",
    "for result in month_rule_list_a:\n",
    "    if not all(value is None for value in result) and result[0] is not None:\n",
    "        column_names = result[0].columns\n",
    "        data_values_list = result[0][column_names].values\n",
    "        additional_data = {\n",
    "            'Indicator': data_values_list,\n",
    "            'RisingMonth': 0,\n",
    "            '유형': '월별규칙성'  # 모든 결과에 대해 '유형'을 '월별규칙성'으로 설정\n",
    "        }\n",
    "        month_rule_list += process_results([result], additional_data=additional_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_10972\\1875387929.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 리스트와 유형을 매핑\n",
    "lists_and_types = [\n",
    "    (select_list[0], '일별급상승'),\n",
    "    (select_list[1], '주별급상승'),\n",
    "    (select_list[2], '월별급상승'),\n",
    "    (rising_list[0], '주별지속상승'),\n",
    "    (rising_list[1], '월별지속상승'),\n",
    "    (month_rule_list, '월별규칙성')\n",
    "]\n",
    "\n",
    "\n",
    "# 모든 리스트를 처리하고 하나의 데이터프레임으로 병합\n",
    "processed_dfs = [utils.process_and_concat(df_list, label) for df_list, label in lists_and_types]\n",
    "\n",
    "# 비어 있지 않은 DataFrame들만 병합\n",
    "graph_result = pd.concat([df for df in processed_dfs if not df.empty]).reset_index(drop=True)\n",
    "\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "# 불필요한 컬럼 삭제 및 '주간지속상승'을 '주별지속상승'으로 수정\n",
    "\n",
    "graph_result = graph_result.drop(columns=['InfoData'])\n",
    "graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n",
    "\n",
    "# 정렬\n",
    "graph_result.sort_values(by=['연관검색어', '유형', '검색일자'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "# 최종 결과 출력\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_and_lists = [\n",
    "    (\"일별 급상승\", select_list[0]),\n",
    "    (\"주별 급상승\", select_list[1]),\n",
    "    (\"주별 지속상승\", rising_list[0]),\n",
    "    (\"월별 급상승\", select_list[2]),\n",
    "    (\"월별 지속상승\", rising_list[1]),\n",
    "    (\"월별 규칙성\", month_rule_list),\n",
    "]\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # data_list가 리스트인지 확인\n",
    "    if not isinstance(data_list, list):\n",
    "        print(f\"{flag_name}: data_list가 리스트가 아닙니다.\")\n",
    "        continue\n",
    "    \n",
    "    # data_list 내의 각 요소가 DataFrame인지, '연관검색어' 컬럼이 있는지 확인\n",
    "    for idx, df in enumerate(data_list):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"{flag_name}: 인덱스 {idx}에 DataFrame이 아닌 요소가 있습니다.\")\n",
    "        elif \"연관검색어\" not in df.columns:\n",
    "            print(f\"{flag_name}: 인덱스 {idx}의 DataFrame에 '연관검색어' 컬럼이 없습니다.\")\n",
    "\n",
    "# utils.update_keywords_flag 함수를 호출하기 전에 각 data_list의 유효성 검사\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # 데이터 프레임으로 구성된 리스트만 유지\n",
    "    valid_data_list = [df for df in data_list if isinstance(df, pd.DataFrame) and \"연관검색어\" in df.columns]\n",
    "    \n",
    "    # 유효한 데이터 리스트만을 사용하여 키워드 플래그 업데이트\n",
    "    utils.update_keywords_flag(collected_keywords_data, valid_data_list, flag_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " # process_data : 지정된 조건에 따라 데이터를 필터링하고, 추가 처리를 통해 최종 데이터프레임을 반환하는 함수.\n",
    "\n",
    "info_result_daily_select = utils.process_data(collected_keywords_data, '일별 급상승', '일별 급상승', select_list[0])\n",
    "info_result_weekly_select = utils.process_data(collected_keywords_data, '주별 급상승', '주별 급상승', select_list[1])\n",
    "info_result_monthly_select = utils.process_data(collected_keywords_data, '월별 급상승', '월별 급상승', select_list[2]) \n",
    "\n",
    "info_result_weekly_continuous = utils.process_data(collected_keywords_data, '주별 지속상승', '주별 지속상승', rising_list[0])\n",
    "\n",
    "info_result_monthly_continuous = utils.process_data(collected_keywords_data, '월별 지속상승', '월별 지속상승', rising_list[1])\n",
    "\n",
    "info_result_monthly_pattern = utils.process_data(collected_keywords_data, '월별 규칙성', '월별 규칙성', month_rule_list)\n",
    "\n",
    "info_result_final = pd.concat([info_result_daily_select,info_result_weekly_select, info_result_monthly_select,\\\n",
    "                               info_result_weekly_continuous, info_result_monthly_continuous,\\\n",
    "                                  info_result_monthly_pattern]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구글/ 네이버 한꺼번에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 뉴스링크,제목 수집 (네이버)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def collect_google_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"google_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            rising_keywords_results = pickle.load(file)\n",
    "    else:\n",
    "        rising_keywords_results = await collect_rising_keywords(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(rising_keywords_results, file)\n",
    "    \n",
    "    return rising_keywords_results\n",
    "\n",
    "async def collect_news_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"news_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            news_data = pickle.load(file)\n",
    "    else:\n",
    "        news_data = await main_news(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(news_data, file)\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# 메인 비동기 실행 함수\n",
    "async def main(target_keywords):\n",
    "    google_keywords_results, news_keywords_results = await asyncio.gather(\n",
    "        collect_google_keywords(target_keywords),\n",
    "        collect_news_keywords(target_keywords)\n",
    "    )\n",
    "    \n",
    "    return google_keywords_results, news_keywords_results\n",
    "target_keywords = list(set(info_result_final['연관키워드']))\n",
    "\n",
    "rising_keywords_results,news_data=asyncio.run(main(target_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'보험상담': [],\n",
       " '간병보험': [],\n",
       " '의료실비보험비교사이트': [],\n",
       " '청년보험': [],\n",
       " '상속세계산': [],\n",
       " '국제조세': [],\n",
       " '코인종류': [],\n",
       " '상속세세율': [],\n",
       " '반도체ETF': [],\n",
       " '전세대출금리비교': [],\n",
       " '상속세세무사': [],\n",
       " '약정서': [],\n",
       " '금거래소시세': [],\n",
       " '미국주식강의': [],\n",
       " '파킹통장': [],\n",
       " '14K금시세': [],\n",
       " '가상자산': [],\n",
       " '오늘순금가격': [],\n",
       " '건강종합보험': [],\n",
       " '차용증': [],\n",
       " '코인만들기': [],\n",
       " 'ISA계좌이전': [],\n",
       " '디오라마제작': [],\n",
       " '퇴직연금담보대출': [],\n",
       " '순금시세': [],\n",
       " '코인상장': [],\n",
       " 'ISA계좌수수료': [],\n",
       " 'NPL': [],\n",
       " '중개형ISA계좌개설': [],\n",
       " '금융IT': [],\n",
       " '코인시세': [],\n",
       " '소득공제용연금저축': [],\n",
       " '세무대리인': ['삼 쩜삼', '삼 쩜삼 세무 대리인 해임', '홈택스 세무 대리인 해지', '홈 텍스', '삼 쩜삼 탈퇴'],\n",
       " '상속증여변호사': [],\n",
       " '현대아이파크분양': [],\n",
       " '트레이딩뷰': [],\n",
       " '비트코인하는법': [],\n",
       " '개인연금ETF': [],\n",
       " '종합보험': ['고용 보험', '자동차 보험', '현대 해상'],\n",
       " '연금보험추천': [],\n",
       " '가사소송': [],\n",
       " '종합보험추천': [],\n",
       " '주식투자대회': [],\n",
       " '유료주식': [],\n",
       " '컴퓨터주식': [],\n",
       " 'ISA증권사비교': [],\n",
       " '공모전사이트': [],\n",
       " 'CMA통장': ['isa 통장', '미래에셋 증권', 'isa 계좌', '파킹 통장'],\n",
       " 'S&P500ETF': [],\n",
       " 'ISA이벤트': ['삼성 증권 이벤트', '미래에셋 isa 이벤트', '키움 증권', 'isa'],\n",
       " '신용카드발급': [],\n",
       " '간병인보험': ['삼성 화재', '간병인 보험 비용', '메리츠 간병인 보험'],\n",
       " '코인정보': ['비트 코인 시세',\n",
       "  '코인 뉴스',\n",
       "  '코인 사이트',\n",
       "  '월드 코인',\n",
       "  '코인 갤러리',\n",
       "  '빗썸',\n",
       "  '비트 코인 가격',\n",
       "  '코인 정보 사이트',\n",
       "  '코인 갤',\n",
       "  '코인 원',\n",
       "  '바이 낸스',\n",
       "  '정보 처리 기사',\n",
       "  '시바이 누',\n",
       "  '코인 마켓 캡',\n",
       "  '시바이 누 코인',\n",
       "  '도지 코인',\n",
       "  '알트 코인 정보'],\n",
       " '카드현금지급': [],\n",
       " '프리랜서대출': [],\n",
       " '증여세신고': ['홈 텍스', '증여세 계산', '홈택스', '증여세 신고 방법', '증여 신고'],\n",
       " '종합건강보험': ['홈택스', '건강 보험 자격 득실 확인서', '건강 보험 심사 평가원'],\n",
       " '차용증법적효력': ['차용증 법적 효력'],\n",
       " '주식장': ['미국 주식 갤러리',\n",
       "  '미국 주식 서머 타임',\n",
       "  '미국 주식 프리 장',\n",
       "  '미국 프리 장',\n",
       "  '엔비디아 주식',\n",
       "  '엔비디아',\n",
       "  '나스닥',\n",
       "  '애플',\n",
       "  '삼성 증권',\n",
       "  '미국 주식 프리 마켓',\n",
       "  '해외 주식',\n",
       "  '미국 장 시간'],\n",
       " '부동산명함': ['부동산 명함'],\n",
       " '주식게임': ['엔비디아 주식',\n",
       "  '네이버 주식',\n",
       "  '테슬라 주가',\n",
       "  '카카오 게임',\n",
       "  '카카오 게임 주식',\n",
       "  '테슬라 주식',\n",
       "  '게임 스탑 주식',\n",
       "  '게임 주',\n",
       "  '해외 주식',\n",
       "  '엔비디아',\n",
       "  '넥슨',\n",
       "  '애플 주식',\n",
       "  '테슬라',\n",
       "  '주식 갤러리'],\n",
       " '삼성ISA계좌': ['중개 형 isa', 'cma 계좌', 'isa 계좌 추천'],\n",
       " '카드이벤트': ['삼성 카드 캐시백',\n",
       "  'bc 바로 카드',\n",
       "  '네이버 페이',\n",
       "  'kb 국민 카드 에서 제공 하는 해외 직구 혜택 모음 이벤트 는 무엇 일까요 준법 감시인 심의 필 240307 00899 hmv',\n",
       "  'bc 카드',\n",
       "  '하나 카드 이벤트',\n",
       "  'kb 카드',\n",
       "  'kb 국민 카드 에서 제공 하는 해외 직구 혜택 모음 이벤트 는 무엇 일까요',\n",
       "  '국민 카드 이벤트',\n",
       "  '국민 카드',\n",
       "  '카드 고릴라',\n",
       "  '신한 카드 이벤트'],\n",
       " '블로그리뷰알바': [],\n",
       " '간이금형': [],\n",
       " '비트코인거래소': [],\n",
       " '질병입원비': [],\n",
       " '채권추심': ['채권 추심 절차', '채권 압류 및 추심 명령 신청서', '신용 정보 회사', '전부 명령', '제 3 채무자'],\n",
       " '생명보험': [],\n",
       " '비트코인지갑': [],\n",
       " '상가임대': [],\n",
       " '금한돈값시세': [],\n",
       " '캐나다여름캠프': [],\n",
       " '토스이벤트': [],\n",
       " '상속상담': [],\n",
       " 'ELS': [],\n",
       " '자동차보험비교': [],\n",
       " '적금추천': [],\n",
       " '배당ETF': [],\n",
       " '배당금계산기': [],\n",
       " '주식검색기': [],\n",
       " '비갱신암보험': ['암 보험'],\n",
       " '코인광고': [],\n",
       " '자동차보험포인트': [],\n",
       " '금시세1돈': [],\n",
       " '부동산홈페이지': [],\n",
       " '코인거래소': [],\n",
       " '크립토': [],\n",
       " '해외지수거래': [],\n",
       " '신용정보채권추심': [],\n",
       " '세무법인': [],\n",
       " '3D프린터가격': [],\n",
       " '영화투자': [],\n",
       " '주식프로그램추천': [],\n",
       " '실패하는사람들의10가지습관': [],\n",
       " '상속증여상담': [],\n",
       " '데시앙분양': [],\n",
       " '금투자': [],\n",
       " '상속변호사': [],\n",
       " '제2금융권대출': [],\n",
       " '대부업체': [],\n",
       " '전자지갑': [],\n",
       " '아파트월세': [],\n",
       " '오늘의금값': [],\n",
       " '부가가치세강의': [],\n",
       " '비갱신형암보험': [],\n",
       " '증권사수수료비교': [],\n",
       " '금시세': [],\n",
       " '리워드광고': [],\n",
       " '자동차가족보험': [],\n",
       " '교보증권해외선물수수료': [],\n",
       " '연금저축계좌': [],\n",
       " '미술갤러리': [],\n",
       " '금융치료클래스': [],\n",
       " '주식투자': [],\n",
       " '사이버대학부동산학과': [],\n",
       " '에스원블루에셋': [],\n",
       " 'DB수집': [],\n",
       " '종합자동차보험': [],\n",
       " '법무사': [],\n",
       " '토지경매송곳투자법': [],\n",
       " 'WEB3ID': [],\n",
       " '노무법인': [],\n",
       " '국민카드발급이벤트': [],\n",
       " '해외선물수수료이벤트': [],\n",
       " '금값시세': ['오늘 의 금값',\n",
       "  '오늘 의 금값 시세',\n",
       "  '금 시세 팔때',\n",
       "  '금 시세 차트',\n",
       "  '금값 차트',\n",
       "  '비트 코인 시세',\n",
       "  '오늘 금 시세 팔때',\n",
       "  '오늘 의 금 시세',\n",
       "  '금 팔때 가격',\n",
       "  '금값 전망',\n",
       "  'gold price',\n",
       "  '현재 금 1 돈 가격',\n",
       "  '금값 오늘 시세',\n",
       "  '금 시세'],\n",
       " '하나은행IRP': ['irp 계좌', '개인 형 irp', 'irp 계좌 개설', '하나 은행'],\n",
       " '퇴직금담보대출': [],\n",
       " '손해보험': ['하나 손해 보험 운전자 보험',\n",
       "  '신한 라이프',\n",
       "  '한화 손해 보험 다이렉트',\n",
       "  '가족 관계 증명서',\n",
       "  '농협 손해 보험 채용',\n",
       "  '메리츠 화재 고객 센터',\n",
       "  'db 손해 보험 가상 pc',\n",
       "  'kb 손해 보험 배구',\n",
       "  '엠지 손해 보험',\n",
       "  '롯데 손해 보험 다이렉트',\n",
       "  '하나 손해 보험 채용',\n",
       "  '보험 개발원',\n",
       "  'kb 손해 보험 홈페이지',\n",
       "  '현대 해상 다이렉트',\n",
       "  '에이스 손해 보험 해지',\n",
       "  '한화 손해 보험 고객 센터',\n",
       "  '산업 안전 보건법 은 원칙적 으로 모든 사업 에 적용 한다',\n",
       "  'db 손해 보험 전화 번호',\n",
       "  '삼성 화재 다이렉트',\n",
       "  'db 자동차 보험 다이렉트'],\n",
       " '건강보험추천': ['건강 보험 공단'],\n",
       " '해외선물거래': ['해외 선물 하는 법', '코인 선물 거래', '해외 선물 거래 시간', '바이 낸스'],\n",
       " '캐피탈': ['엘엔 에스 벤처 캐피탈',\n",
       "  'nh 농협 캐피탈',\n",
       "  '현대 캐피탈 채용',\n",
       "  'ok 캐피탈',\n",
       "  '애 큐온 캐피탈',\n",
       "  '바로 바로 론',\n",
       "  '키움 캐피탈',\n",
       "  '프랙 시스 캐피탈',\n",
       "  '에이원 대부 캐피탈',\n",
       "  '케이비 캐피탈',\n",
       "  '리딩 에이스 캐피탈',\n",
       "  '현대 커머셜',\n",
       "  '애플 론',\n",
       "  '메리츠 캐피탈',\n",
       "  '현대 캐피탈 자동차',\n",
       "  '한국 벤처 캐피탈 협회',\n",
       "  '벤처 캐피탈 협회',\n",
       "  'vc'],\n",
       " '한국화가그림': [],\n",
       " '퇴직연금IRP': ['하나 은행 irp',\n",
       "  '미래에셋 irp',\n",
       "  '연금 저축 계좌',\n",
       "  '신한 은행',\n",
       "  '신한 은행 irp',\n",
       "  '하나 은행',\n",
       "  '연금 저축 세액 공제',\n",
       "  'isa 계좌',\n",
       "  '퇴직 연금 중도 인출',\n",
       "  '퇴직 연금 연말 정산',\n",
       "  'irp 수수료',\n",
       "  '퇴직 연금 수령',\n",
       "  '퇴직 연금 dc 형',\n",
       "  '개인 형 irp',\n",
       "  'irp 계좌 개설'],\n",
       " '사모펀드': [],\n",
       " '고배당주식': [],\n",
       " '은행달러환율': [],\n",
       " '배당금높은주식': [],\n",
       " '자동주식매매': [],\n",
       " '미국주식': [],\n",
       " '미수금내용증명': [],\n",
       " '파라곤분양': [],\n",
       " '인터넷세무사': [],\n",
       " '장외채권': [],\n",
       " '비상장주식거래소': [],\n",
       " '코인상담': [],\n",
       " '퇴직연금수령방법': [],\n",
       " '롯데더알찬건강보험': [],\n",
       " '명함제작': [],\n",
       " '상장컨설팅': [],\n",
       " '자동차보험': [],\n",
       " 'KB증권중개형ISA': [],\n",
       " '보험료조회': [],\n",
       " 'CMA': [],\n",
       " '비트버니': [],\n",
       " '미국주식추천': [],\n",
       " '비트코인관련주': [],\n",
       " '상속전문세무사': [],\n",
       " '손해평가사': [],\n",
       " 'ISA중개형증권사': [],\n",
       " '개인형IRP': [],\n",
       " '블록체인지갑': [],\n",
       " 'ETF추천': [],\n",
       " '은퇴': [],\n",
       " '사업자등록증': [],\n",
       " '노무사비용': [],\n",
       " '소득세법': [],\n",
       " '금펀드': [],\n",
       " '주식기초강의': [],\n",
       " '세무': [],\n",
       " '노인자동차보험': [],\n",
       " '상속세계산기': [],\n",
       " '퇴직연금': [],\n",
       " '종이신문': [],\n",
       " '중개형ISA계좌': [],\n",
       " '비과세예금': [],\n",
       " '대부업': [],\n",
       " '중개형ISA비교': [],\n",
       " '버그클라우드': [],\n",
       " 'CMA금리비교': [],\n",
       " '뱅크샐러드카드이벤트': [],\n",
       " '아파트': [],\n",
       " '뱅크샐러드롯데카드': [],\n",
       " '오늘금1돈시세': [],\n",
       " '상속세신고수수료': [],\n",
       " '투자종류': [],\n",
       " '건강보험상담': [],\n",
       " '간병비보험': [],\n",
       " '분양시행사': [],\n",
       " 'CI보험': [],\n",
       " '소품제작': [],\n",
       " '자산관리교육': [],\n",
       " '코인추천': [],\n",
       " '무료주식': [],\n",
       " '주식교육': [],\n",
       " '코인개발': [],\n",
       " '차용증양식': [],\n",
       " '금값가격': [],\n",
       " '코인가격': [],\n",
       " '아파트청약': [],\n",
       " '암보험비갱신형': [],\n",
       " '자동자보험비교': [],\n",
       " '코인선물': [],\n",
       " '수요조사': [],\n",
       " '홍콩H지수': [],\n",
       " '상속세전문': [],\n",
       " '순금': [],\n",
       " 'ETF': [],\n",
       " '와디즈상세페이지': [],\n",
       " '주식정보사이트': [],\n",
       " '전고체배터리관련주': [],\n",
       " '그림매매': [],\n",
       " '자동차보험료비교견적': [],\n",
       " 'FECR': [],\n",
       " '카드사캐시백이벤트': [],\n",
       " 'ISA추천': [],\n",
       " '코인투자방법': [],\n",
       " '등기': [],\n",
       " '미국비자대행': [],\n",
       " '다이렉트자동차보험': [],\n",
       " '성년후견인신청': [],\n",
       " '작품구매': [],\n",
       " '타이거ETF': [],\n",
       " 'IPO컨설팅': [],\n",
       " '노후준비': [],\n",
       " '코인마케팅': [],\n",
       " 'KB증권ISA수수료': [],\n",
       " '상해보험추천': [],\n",
       " '3대질환보험': [],\n",
       " '노무사상담': [],\n",
       " 'ISA증권사': [],\n",
       " '투자추천': [],\n",
       " 'ISA계좌개설이벤트': [],\n",
       " '증여세상담': [],\n",
       " '금값': [],\n",
       " '보험사': [],\n",
       " '개인재무설계': [],\n",
       " '저가매수': [],\n",
       " '명함': [],\n",
       " '카드신규발급혜택': [],\n",
       " '건강보험비교사이트': [],\n",
       " '금형사출': [],\n",
       " 'ISA가입이벤트': [],\n",
       " '지주택분양': [],\n",
       " '월세방': [],\n",
       " '주식사이트': [],\n",
       " '금ETF': [],\n",
       " '상속포기': [],\n",
       " '상속세전문세무사': [],\n",
       " '미국주식이벤트': [],\n",
       " '현물ETF': [],\n",
       " '자동차보험비교견적사이트': [],\n",
       " '개별주식선물': [],\n",
       " '공모전': [],\n",
       " 'ISA이전': [],\n",
       " 'ISA계좌추천': [],\n",
       " '부동산경매공부': [],\n",
       " '부동산매물사이트': [],\n",
       " '수술비보험': [],\n",
       " '상속세상담': [],\n",
       " '뱅크샐러드신용카드': [],\n",
       " 'RAISE3D': [],\n",
       " 'ETF투자방법': [],\n",
       " '주식강의': [],\n",
       " '투자자문회사': [],\n",
       " 'RP': [],\n",
       " '노무사상담비용': [],\n",
       " 'QQQ주가': [],\n",
       " '대출상담사': [],\n",
       " '와디지ㅡ': [],\n",
       " '양도소득세신고': [],\n",
       " '상속분쟁': [],\n",
       " '채무자신용조회': [],\n",
       " '사망보험': [],\n",
       " '다모아보험': [],\n",
       " '소득세계산법': [],\n",
       " '18K금가격': [],\n",
       " '코인거래방법': [],\n",
       " '건강보험비교': [],\n",
       " '오늘에금값': [],\n",
       " 'AI관련주': [],\n",
       " '금값시세한돈': [],\n",
       " 'ETF투자': [],\n",
       " '해외주식': [],\n",
       " '주식차트사이트': [],\n",
       " '종합보험가입': [],\n",
       " '오늘의금시세한돈': [],\n",
       " '건강보험다이렉트': [],\n",
       " '어린이카드': [],\n",
       " '돈버는어플': [],\n",
       " '코인투자': [],\n",
       " '상속세신고': [],\n",
       " '미국환율': [],\n",
       " '이자율계산기': [],\n",
       " '전세권설정': [],\n",
       " '자동차다이렉트보험': [],\n",
       " '파킹통장금리비교': [],\n",
       " '한싹청약': [],\n",
       " '비상장주식거래': [],\n",
       " '고배당주': [],\n",
       " 'ISA증권': [],\n",
       " '즉시연금보험': [],\n",
       " '비트코인시세': [],\n",
       " '실손보험가입': [],\n",
       " '오늘의금값시세': [],\n",
       " '월세': [],\n",
       " '더샾분양': [],\n",
       " '한정승인': [],\n",
       " '금시세조회': [],\n",
       " 'ISA계좌개설': [],\n",
       " '캐쉬백신용카드': [],\n",
       " '3DPRINTER': [],\n",
       " '코인지갑': [],\n",
       " '코인사이트': [],\n",
       " '자동차보험들기': [],\n",
       " '3대질병진단비보험': [],\n",
       " 'IRP계좌': [],\n",
       " '산업용3D프린터': [],\n",
       " '차용': [],\n",
       " '18K금시세': [],\n",
       " '대출이자계산기': [],\n",
       " '백금시세': [],\n",
       " 'SSAI': [],\n",
       " '미국방학캠프': [],\n",
       " '지급명령신청': [],\n",
       " '알바': [],\n",
       " '법인등기': [],\n",
       " '사업자등록': [],\n",
       " '목돈굴리기': [],\n",
       " '상장주식': [],\n",
       " '롯데카드뱅크샐러드': [],\n",
       " '상속소송': [],\n",
       " '엔비디아주가': [],\n",
       " '금팔때시세': [],\n",
       " '마케팅툴': [],\n",
       " 'KB증권ISA이벤트': [],\n",
       " 'WWW.SAMSUNGFIRE.COM': [],\n",
       " '펀드비교': [],\n",
       " '상속전문변호사': [],\n",
       " '해외선물수수료': [],\n",
       " 'TSP': [],\n",
       " '12월공모주달력': [],\n",
       " '자동차보험비교견적': [],\n",
       " '자동차책임보험': [],\n",
       " '경제신문': [],\n",
       " '암보험': [],\n",
       " '미국주식시간': [],\n",
       " '블록체인컨설팅': [],\n",
       " '서울사이버대학교부동산학과': [],\n",
       " '해외선물': [],\n",
       " '금매도가격': [],\n",
       " '해외선물실전투자대회': [],\n",
       " '그림사는곳': [],\n",
       " '실비보험가입': [],\n",
       " '알트코인': [],\n",
       " '자동차공동명의보험': [],\n",
       " 'ETF종류': [],\n",
       " '재테크교육': [],\n",
       " 'ISA거래수수료': [],\n",
       " '오늘금시세': [],\n",
       " '사업자등록증발급': [],\n",
       " '배당주': [],\n",
       " 'IRP계좌개설': [],\n",
       " '투자플랫폼': [],\n",
       " '24K금값': [],\n",
       " '등록부정정': [],\n",
       " 'ISA증권사추천': [],\n",
       " '금시세18K': [],\n",
       " '현재금값': [],\n",
       " '특징주': [],\n",
       " '금값시세1돈': [],\n",
       " '신용카드신규발급혜택': [],\n",
       " 'AI주식': [],\n",
       " '부동산경매강의': [],\n",
       " '자동차보험료비교': [],\n",
       " '증여전문변호사': [],\n",
       " '타사대체이벤트': [],\n",
       " '고가그림': [],\n",
       " '금가격1돈': [],\n",
       " '3D프린터기': [],\n",
       " 'POLY': [],\n",
       " '상속세율': [],\n",
       " '부동산크라우드펀딩': [],\n",
       " 'HTS추천': [],\n",
       " '코인차트': [],\n",
       " '신진작가그림': [],\n",
       " '레진제작': [],\n",
       " '코인하는법': [],\n",
       " '자동차보험저렴한곳': [],\n",
       " '자동차보험료비교견적사이트': [],\n",
       " '미국주식사는법': [],\n",
       " '증여세율': [],\n",
       " '부가세신고교육': [],\n",
       " '미술품대여': [],\n",
       " '실비보험비갱신': []}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rising_keywords_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 ./data/target_keywords/240315\\keyword_activity_rates.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "#활동성 분석\n",
    "################################\n",
    "today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "directory_path = f\"./data/target_keywords/{today_date}\"\n",
    "file_path = os.path.join(directory_path, \"target_keywords.txt\")\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    # 디렉토리가 존재하지 않는 경우, 디렉토리 생성\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# 파일이 존재하는지 확인\n",
    "if not os.path.exists(file_path):\n",
    "    # 키워드를 파일에 작성\n",
    "    with open(file_path, 'w') as file:\n",
    "        for keyword in target_keywords:\n",
    "            file.write(\"%s\\n\" % keyword)\n",
    "    result = f\"{file_path}에 키워드 저장됨\"\n",
    "else:\n",
    "    result = f\"{file_path} 파일이 이미 존재합니다. 작업을 건너뜁니다.\"\n",
    "\n",
    "result\n",
    "\n",
    "\n",
    "main_blog_analysis(f'{directory_path}/target_keywords.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# 뉴스링크,제목,연관검색어 데이터프레임 생성\n",
    "#######################################\n",
    "\n",
    "\n",
    "name_list = list(news_data.keys())  \n",
    "# DataFrame 초기화\n",
    "news_df = pd.DataFrame()\n",
    "\n",
    "# 모든 키워드에 대해 처리\n",
    "for keyword in name_list:\n",
    "    # 뉴스 항목이 있는 경우 데이터 추가\n",
    "    for news_item in news_data[keyword]:\n",
    "        news_row = [keyword, news_item[0], news_item[1]]  # 연관키워드, 뉴스제목, 뉴스링크\n",
    "        news_df = pd.concat([news_df, pd.DataFrame([news_row])], ignore_index=True)\n",
    "\n",
    "    # 뉴스 항목 수가 10개에 미치지 못하면 나머지를 빈 행으로 채움\n",
    "    for _ in range(10 - len(news_data[keyword])):\n",
    "        empty_row = [keyword, None, None]  # 연관키워드, 빈 뉴스제목, 빈 뉴스링크\n",
    "        news_df = pd.concat([news_df, pd.DataFrame([empty_row])], ignore_index=True)\n",
    "\n",
    "# 칼럼 이름 설정\n",
    "news_df.columns = ['연관검색어', '뉴스제목', '뉴스링크']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_activity_rates = pd.read_csv(f'{directory_path}/keyword_activity_rates.csv')\n",
    "keyword_activity_rates.columns = ['연관검색어', '활동성']\n",
    "\n",
    "# '활동성' 열의 데이터를 백분율 형태의 문자열로 변환\n",
    "keyword_activity_rates['활동성'] = keyword_activity_rates['활동성'].apply(lambda x: f\"{x}%\")\n",
    "# news_df와 keyword_activity_rates를 '연관검색어' 열을 기준으로 병합\n",
    "keyword_activity_rates = keyword_activity_rates.drop_duplicates(subset=['연관검색어'])\n",
    "merged_keyword_activity_rates = pd.merge(news_df, keyword_activity_rates, on='연관검색어', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "# 네이버 merge\n",
    "####\n",
    "collected_keywords_dat_copy.rename(columns={'연관키워드': '연관검색어'}, inplace=True)\n",
    "info_result_final.rename(columns={'연관키워드': '연관검색어'}, inplace=True)\n",
    "# collected_keywords_dat_copy에서 '연관키워드'와 '검색어'를 기준으로 중복 제거\n",
    "collected_keywords_dat_copy = collected_keywords_dat_copy.drop_duplicates(subset=['연관검색어'], keep='first')\n",
    "# 이제 merged_keyword_activity_rates와 결합\n",
    "final_merged_df = pd.merge(merged_keyword_activity_rates, collected_keywords_dat_copy[['연관검색어', '검색어']], on='연관검색어', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_29456\\1304442105.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_keywords_by_search = collected_keywords_dat_copy.groupby('검색어').apply(\n"
     ]
    }
   ],
   "source": [
    "final_merged_df_copy = final_merged_df.copy()\n",
    "\n",
    "# 구글검색어 컬럼을 초기화합니다.\n",
    "final_merged_df_copy['구글검색어'] = None\n",
    "\n",
    "# 이후의 모든 작업은 final_merged_df_copy에 대해 수행합니다.\n",
    "i = 0\n",
    "for keyword, queries in rising_keywords_results.items():\n",
    "    filled_queries = queries[:10] + [None] * (10 - len(queries[:10]))\n",
    "    for query in filled_queries:\n",
    "        if i < len(final_merged_df_copy):\n",
    "            final_merged_df_copy.at[i, '구글검색어'] = query\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "# final_merged_df의 '검색어' 컬럼에서 각 10번째 검색어를 추출합니다.\n",
    "keyword_list_per_10 = final_merged_df_copy['검색어'].tolist()[::10]\n",
    "\n",
    "\n",
    " \n",
    "# collected_keywords_dat_copy에서 각 검색어별 상위 10개 연관검색어를 가져옵니다.\n",
    "# 여기서는 각 검색어별로 가장 높은 월간검색수를 가진 상위 10개를 선정합니다.\n",
    "top_keywords_by_search = collected_keywords_dat_copy.groupby('검색어').apply(\n",
    "    lambda x: x.nlargest(10, '월간검색수_합계')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# 새로운 DataFrame을 초기화합니다. 이 DataFrame에는 각 검색어별 상위 10개 연관검색어가 포함됩니다.\n",
    "new_rows_for_final_df = []\n",
    "\n",
    "\n",
    "for keyword in keyword_list_per_10:\n",
    "    # 특정 키워드에 대한 상위 10개 연관 검색어 추출\n",
    "    top_queries_for_keyword = top_keywords_by_search[top_keywords_by_search['검색어'] == keyword].head(10)\n",
    "    \n",
    "    # 추출된 연관 검색어를 결과 리스트에 추가\n",
    "    num_rows_added = 0  # 추가된 연관 검색어의 수를 추적\n",
    "    for _, row in top_queries_for_keyword.iterrows():\n",
    "        new_rows_for_final_df.append(row['연관검색어'])\n",
    "        num_rows_added += 1\n",
    "    \n",
    "    # 10개 미만인 경우 나머지를 None으로 채우기\n",
    "    for _ in range(10 - num_rows_added):\n",
    "        new_rows_for_final_df.append(None)\n",
    "\n",
    "\n",
    "# new_rows_for_final_df의 길이를 확인하고 final_merged_df의 '네이버검색어' 컬럼에 값을 할당합니다.\n",
    "# 주의: new_rows_for_final_df의 길이가 final_merged_df의 행 수와 동일해야 합니다.\n",
    "# 만약 길이가 다르다면, 길이가 맞도록 조정이 필요합니다.\n",
    "if len(new_rows_for_final_df) == len(final_merged_df_copy):\n",
    "    final_merged_df_copy['네이버검색어'] = new_rows_for_final_df\n",
    "else:\n",
    "    print(\"경고: '네이버검색어' 데이터의 길이가 final_merged_df와 다릅니다. 데이터 확인이 필요합니다.\")\n",
    "\n",
    "# 최종 DataFrame 확인\n",
    "#final_merged_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형식 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_29456\\2013373558.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_29456\\2013373558.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['연관검색어'] = info_result_af_copy_reordered['연관검색어'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_29456\\2013373558.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['네이버검색어'] = info_result_af_copy_reordered['네이버검색어'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_29456\\2013373558.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['구글검색어'] = info_result_af_copy_reordered['구글검색어'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_29456\\2013373558.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"–\", \"-\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#info_result_final = info_result_final.drop(columns=[\"일별 급상승\", \"주별 급상승\", \"주별 지속상승\", \"월별 급상승\", \"월별 지속상승\", \"월별 규칙성\"])\n",
    "\n",
    "final_merged_df_result = pd.merge(info_result_final, final_merged_df_copy, how='left', on='연관검색어')\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# '기준일자' 컬럼을 가장 앞에 추가\n",
    "final_merged_df_result.insert(0, '기준일자', today_date)\n",
    "# 컬럼명 변경: '중복검색어' -> '검색키워드', '월간검색수_합계' -> '검색량'\n",
    "\n",
    "final_merged_df_result.rename(columns={'중복검색어': '검색키워드', '월간검색수_합계': '검색량'}, inplace=True)\n",
    "\n",
    "final_merged_df_result = final_merged_df_result.drop(columns=[\"검색어\"])\n",
    "\n",
    "\n",
    "final_merged_df_result['상승월'] = None\n",
    "# rising_month_list의 각 항목에 대해 반복 처리\n",
    "for month_info in rising_month_list:\n",
    "    months, keyword = month_info  # month_info는 각각의 월 목록과 키워드를 포함합니다.\n",
    "    keyword_rows = final_merged_df_result[final_merged_df_result['연관검색어'] == keyword]  # 해당 키워드에 대한 행만 선택합니다.\n",
    "    \n",
    "    if not keyword_rows.empty:\n",
    "\n",
    "        for i, month in enumerate(months):\n",
    "            if i < len(keyword_rows):\n",
    "                final_merged_df_result.loc[keyword_rows.index[i], '상승월'] = month\n",
    "            else:\n",
    "                break  # 월의 개수보다 더 많은 행에 대해서는 처리를 중단합니다.\n",
    "\n",
    "\n",
    "\n",
    "# 형식맞추기 위한 info_result_final 순서 정렬\n",
    "info_result_af_copy=pd.DataFrame()\n",
    "a = final_merged_df_result.query(\"`유형` == '일별 급상승'\")\n",
    "b = final_merged_df_result.query(\"`유형` == '주별 급상승' or `유형` == '주별 지속상승'\")\n",
    "c = final_merged_df_result.query(\"`유형` == '월별 급상승' or `유형` == '월별 지속상승' or `유형` == '월별 규칙성'\")\n",
    "a_sort=a.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "b_sort = b.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "c_sort = c.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "info_result_af_copy=pd.concat([a_sort,b_sort,c_sort])\n",
    "\n",
    "# 형식을 위한 이름 변경\n",
    "new_column_order = ['기준일자', '유형', '연관검색어', '검색키워드', '검색량', '지표', '뉴스제목', '뉴스링크', '활동성', '구글검색어', '네이버검색어', '상승월']\n",
    "info_result_af_copy_reordered = info_result_af_copy[new_column_order]\n",
    "\n",
    "# 혹시나 모를 예외처리(형식에 어긋나는 것을 예방)\n",
    "info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['연관검색어'] = info_result_af_copy_reordered['연관검색어'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['네이버검색어'] = info_result_af_copy_reordered['네이버검색어'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['구글검색어'] = info_result_af_copy_reordered['구글검색어'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"–\", \"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유형 순서 정렬\n",
    "info_result_af_copy_reordered_modified = info_result_af_copy_reordered.copy()\n",
    "\n",
    "\n",
    "# 인덱스 재설정\n",
    "info_result_af_copy_reordered_modified.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sort_order = {\n",
    "    \"일별 급상승\": 1,\n",
    "    \"주별 급상승\": 2,\n",
    "    \"주별 지속상승\": 3,\n",
    "    \"월별 급상승\": 4,\n",
    "    \"월별 지속상승\": 5,\n",
    "    \"월별 규칙성\" : 6\n",
    "}\n",
    "\n",
    "# 유형 컬럼에 대한 정렬 순서를 적용하기 위해 임시 컬럼 추가\n",
    "info_result_af_copy_reordered_modified['sort_key'] = info_result_af_copy_reordered_modified['유형'].map(sort_order)\n",
    "\n",
    "# 임시 컬럼을 기준으로 정렬\n",
    "info_result_af_copy_reordered_modified = info_result_af_copy_reordered_modified.sort_values(by=['sort_key', '연관검색어'], ascending=[True, True])\n",
    "\n",
    "# 임시 컬럼 삭제\n",
    "info_result_af_copy_reordered_modified.drop('sort_key', axis=1, inplace=True)\n",
    "\n",
    "info_result_af_copy_reordered_modified.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형식에 맞춰서 띄어쓰기 변경\n",
    "# '유형' 컬럼의 값을 바꾸기 위한 딕셔너리 정의\n",
    "replace_values = {\n",
    "    '일별 급상승': '일별급상승',\n",
    "    '주별 급상승': '주별급상승',\n",
    "    '주별 지속상승': '주별지속상승',\n",
    "    '월별 급상승': '월별급상승',\n",
    "    '월별 지속상승': '월별지속상승',\n",
    "    '월별 규칙성': '월별규칙성'\n",
    "}\n",
    "\n",
    "# '유형' 컬럼 내의 값을 바꾸기\n",
    "graph_result['유형'] = graph_result['유형'].replace(replace_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_related_search_terms = list(graph_result[pd.isna(graph_result['검색량'])]['연관검색어'])\n",
    "unique_na_related_search_terms  = list(set(na_related_search_terms))\n",
    "\n",
    "filtered_graph_result = graph_result[~graph_result['연관검색어'].isin(unique_na_related_search_terms)]\n",
    "\n",
    "\n",
    "filtered_info_result_af_copy_reordered_modified = info_result_af_copy_reordered_modified[~info_result_af_copy_reordered_modified['연관검색어'].isin(unique_na_related_search_terms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info_result_af_out csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at: ./data/result_out\\240315\\info_result_af_out_240315.csv\n"
     ]
    }
   ],
   "source": [
    "column_names = '|||'.join(filtered_info_result_af_copy_reordered_modified[:-1]) + '|||'\n",
    "\n",
    "\n",
    "# 각 행을 '|||'로 구분된 문자열로 변환하고 새로운 DataFrame의 한 컬럼으로 저장\n",
    "df_string = filtered_info_result_af_copy_reordered_modified.apply(lambda x: '|||'.join(x.fillna(' ').astype(str))+'|||', axis=1)\n",
    "\n",
    "new_df = pd.DataFrame(df_string, columns=[column_names])\n",
    "\n",
    "today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "\n",
    "base_save_path = \"./data/result_out\"\n",
    "\n",
    "# 오늘 날짜를 포함한 최종 저장 경로 생성\n",
    "final_save_path = os.path.join(base_save_path, today_date)\n",
    "\n",
    "# 최종 저장 경로가 없는 경우 생성\n",
    "if not os.path.exists(final_save_path):\n",
    "    os.makedirs(final_save_path)\n",
    "\n",
    "# 파일 저장 경로 설정\n",
    "file_save_path = os.path.join(final_save_path, f\"info_result_af_out_{today_date}.csv\")\n",
    "\n",
    "# DataFrame을 CSV 파일로 저장\n",
    "new_df.to_csv(file_save_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"File saved at: {file_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_graph_result_updated = filtered_graph_result.iloc[:, :-2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph_result_out csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at: ./data/result_out\\240315\\graph_result_out_240315.csv\n"
     ]
    }
   ],
   "source": [
    "column_names = '|||'.join(filtered_graph_result_updated.columns[:-1]) +'|||검색량|||'\n",
    "\n",
    "\n",
    "# 각 행을 '|||'로 구분된 문자열로 변환하고 새로운 DataFrame의 한 컬럼으로 저장\n",
    "\n",
    "df_string = filtered_graph_result_updated.apply(lambda x: '|||'.join(x.astype(str)) + '|||', axis=1)\n",
    "new_df = pd.DataFrame(df_string, columns=[column_names])\n",
    "\n",
    "today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "\n",
    "base_save_path = \"./data/result_out\"\n",
    "\n",
    "# 오늘 날짜를 포함한 최종 저장 경로 생성\n",
    "final_save_path = os.path.join(base_save_path, today_date)\n",
    "\n",
    "# 최종 저장 경로가 없는 경우 생성\n",
    "if not os.path.exists(final_save_path):\n",
    "    os.makedirs(final_save_path)\n",
    "\n",
    "# 파일 저장 경로 설정\n",
    "file_save_path = os.path.join(final_save_path, f\"graph_result_out_{today_date}.csv\")\n",
    "\n",
    "# DataFrame을 CSV 파일로 저장\n",
    "new_df.to_csv(file_save_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"File saved at: {file_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
