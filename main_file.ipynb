{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "zip_path = '/content/realdata.zip' \n",
    "extract_to = '/content/'     \n",
    "# ZIP 파일 압축 해제\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "print(\"utils 압축 해제 완료.\")\n",
    "\n",
    "# # 압축 해제할 파일 경로와 압축 해제될 위치 지정\n",
    "# zip_path = '/content/models.zip'  \n",
    "# extract_to = '/content/'   \n",
    "# # ZIP 파일 압축 해제\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# zip_path = '/content/utils.zip' \n",
    "# extract_to = '/content/'     \n",
    "# # ZIP 파일 압축 해제\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to)\n",
    "# print(\"utils 압축 해제 완료.\")\n",
    "\n",
    "\n",
    "# import zipfile\n",
    "# import pandas as pd\n",
    "# from io import BytesIO\n",
    "\n",
    "# # ZIP 파일 경로\n",
    "# archive_path = 'data.zip'\n",
    "\n",
    "# # ZIP 파일 열기 및 파일 목록에서 'info_'로 시작하는 파일 찾기\n",
    "# file_name = None\n",
    "# with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "#     for content in zip_ref.namelist():\n",
    "#         if content.startswith('info_') and content.endswith('.csv'):\n",
    "#             file_name = content\n",
    "#             break\n",
    "\n",
    "# # 찾은 파일 이름을 바탕으로 데이터 로드\n",
    "# if file_name:\n",
    "#     with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "#         with zip_ref.open(file_name) as file:\n",
    "#             info_data_beforeday = pd.read_csv(BytesIO(file.read()))\n",
    "#             print(f\"{file_name} 파일이 성공적으로 로드되었습니다.\")\n",
    "# else:\n",
    "#     print(\"ZIP 파일 내에 'info_'로 시작하는 .csv 파일이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asyncio in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 2)) (3.9.3)\n",
      "Requirement already satisfied: pytrends in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 3)) (4.9.2)\n",
      "Requirement already satisfied: dtw-python in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: google_trends_api in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 5)) (2.2.6)\n",
      "Requirement already satisfied: urllib3<2 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 6)) (1.26.18)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytrends->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytrends->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytrends->-r requirements.txt (line 3)) (4.9.4)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dtw-python->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dtw-python->-r requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google_trends_api->-r requirements.txt (line 5)) (0.26.0)\n",
      "Requirement already satisfied: tenacity in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google_trends_api->-r requirements.txt (line 5)) (8.2.3)\n",
      "Requirement already satisfied: loguru in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google_trends_api->-r requirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.25->pytrends->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.25->pytrends->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.25->pytrends->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.0->pytrends->-r requirements.txt (line 3)) (2023.7.22)\n",
      "Requirement already satisfied: anyio in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->google_trends_api->-r requirements.txt (line 5)) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->google_trends_api->-r requirements.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->google_trends_api->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx->google_trends_api->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from loguru->google_trends_api->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from loguru->google_trends_api->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chohy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends->-r requirements.txt (line 3)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#  버전 설치\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pytz import timezone\n",
    "from datetime import datetime,timedelta\n",
    "import nest_asyncio\n",
    "from utils.api_set import APIClient\n",
    "import utils.utils as utils\n",
    "import utils.formatting as formatting\n",
    "import models.crawling.trend as trend \n",
    "from models.crawling.collect_keywords import collect_keywords\n",
    "from models.crawling.google_trend import collect_rising_keywords\n",
    "from models.naver.news import main_news \n",
    "from models.anaysis import execute_analysis , process_results ,process_results_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. API설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API 설정\n",
    "BASE_URL = utils.get_secret(\"BASE_URL\")\n",
    "CUSTOMER_ID = utils.get_secret(\"CUSTOMER_ID\")\n",
    "API_KEY = utils.get_secret(\"API_KEY\")\n",
    "SECRET_KEY = utils.get_secret(\"SECRET_KEY\")\n",
    "URI = utils.get_secret(\"URI\")\n",
    "METHOD = utils.get_secret(\"METHOD\")\n",
    "# API 클라이언트 인스턴스 생성\n",
    "api_client = APIClient(BASE_URL, CUSTOMER_ID, API_KEY, SECRET_KEY,URI,METHOD)\n",
    "# 키 로드\n",
    "keywords_data = utils.load_keywords('main_keyword.json')\n",
    "\n",
    "# 오늘의 날짜 가져오기\n",
    "formatted_today, day = utils.get_today_date()\n",
    "\n",
    "\n",
    "utils.make_directory('./data')\n",
    "utils.make_directory('./data/rl_srch')\n",
    "utils.make_directory(f'./data/rl_srch/{day}')  # 키워드별 연관검색어 리스트 저장\n",
    "\n",
    "# 검색어 리스트와 결과 저장 경로 설정\n",
    "srch_keyword = ['keyword_final']  \n",
    "save_path = './data/rl_srch/'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 연관검색어 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main(srch_keyword, day):\n",
    "    # 오늘 날짜로 폴더 경로 생성\n",
    "    folder_path = './data/rl_srch/' + datetime.now().strftime('%y%m%d')\n",
    "    file_path = f\"{folder_path}/collected_keywords.csv\"\n",
    "    \n",
    "    # 폴더가 존재하는지 확인\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # 파일이 존재하는지 확인\n",
    "    if os.path.isfile(file_path):\n",
    "        # 파일이 존재하면, 데이터를 읽어옵니다.\n",
    "        collected_keywords_data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 파일이 없으면, collect_keywords 함수를 호출해서 데이터를 수집합니다.\n",
    "        collected_keywords_data = await collect_keywords(srch_keyword, day)\n",
    "        # 결과를 CSV로 저장\n",
    "        collected_keywords_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    return collected_keywords_data\n",
    "collected_keywords_data=asyncio.run(main(srch_keyword, day))\n",
    "\n",
    "collected_keywords_dat_copy=asyncio.run(main(srch_keyword, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 중복검색어컬럼 생성\n",
    "temp_df = utils.generate_unique_search_terms(collected_keywords_dat_copy)\n",
    "\n",
    "## 중복제거하고 50개씩 뽑는 로직 \n",
    "##  check_list: 2250개의 데이터(선별함수에 사용될 검색어별 50개의 연관키워드)를 담고있는 리스트 ,\n",
    "##  collected_keywords_data: 사용할 df\n",
    "collected_keywords_data,check_list = utils.get_top_50_unique_items(collected_keywords_data,temp_df)\n",
    "collected_keywords_data= utils.add_client_info(collected_keywords_data)\n",
    "new_columns = ['일별급상승', '주별급상승', '월별급상승', '주별지속상승', '월별지속상승', '월별규칙성']\n",
    "\n",
    "for column in new_columns:\n",
    "    collected_keywords_data[column] = 0\n",
    "def groupped_df(name,collected_keywords_data):\n",
    "    grouped = collected_keywords_data.groupby(name)\n",
    "    df_list = [group for _, group in grouped]\n",
    "    return df_list\n",
    "df_list=groupped_df('id',collected_keywords_data)\n",
    "n=len(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터를 로드하거나 크롤링하여 반환하는 비동기 함수\n",
    "async def load_or_crawl_data(df_list, clients):\n",
    "    today_date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory = f\"./data/trend_data/{today_date_str}\"\n",
    "    save_path = f\"{directory}/data_{today_date_str}.pkl\"\n",
    "    \n",
    "    # 파일이 존재하면 데이터 로드\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    else:\n",
    "        # 파일이 없으면 비동기 크롤링 시작\n",
    "        results = await run_all(df_list, clients)\n",
    "        # 결과 데이터 저장\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 비동기 크롤링 함수\n",
    "async def trend_main(df, clients):\n",
    "    params = {\n",
    "        \"search_keywords\": list(df['연관키워드']),\n",
    "        \"id\": df['id'].iloc[0],\n",
    "        \"pw\": df['pw'].iloc[0],\n",
    "        \"api_url\": \"https://openapi.naver.com/v1/datalab/search\",\n",
    "        \"name\": '연관검색어'\n",
    "    }\n",
    "    api_url = \"https://openapi.naver.com/v1/datalab/search\"\n",
    "    \n",
    "    # trend_maincode 함수 실행\n",
    "    results = await trend.trend_maincode(params, clients, api_url)\n",
    "    return results\n",
    "\n",
    "async def run_all(df_list, clients):\n",
    "    tasks = [trend_main(df, clients) for df in df_list]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "clients = utils.get_secret(\"clients\")  # clients 정보를 로드\n",
    "\n",
    "# 이벤트 루프 실행 및 데이터 로드 또는 크롤링\n",
    "trend_main_data = asyncio.run(load_or_crawl_data(df_list, clients))\n",
    "results = trend_main_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전역 변수로 리스트 초기화\n",
    "month_rule_list_a = []\n",
    "rising_list_a = [[], []]  # 주별 상승, 월별 상승\n",
    "select_list_a = [[], [], []]  # 일별 선택, 주별 선택, 월별 선택\n",
    "execute_analysis(results,month_rule_list_a,rising_list_a,select_list_a)\n",
    "\n",
    "\n",
    "# month_rule_list=[]\n",
    "select_list=[[],[],[]]\n",
    "\n",
    "rising_list=[[],[]]\n",
    "rising_month_list=[]\n",
    "\n",
    "\n",
    "# 각 리스트를 처리\n",
    "select_list[0] = process_results(select_list_a[0])\n",
    "select_list[1] = process_results(select_list_a[1])\n",
    "select_list[2] = process_results(select_list_a[2])\n",
    "\n",
    "rising_list[0] = process_results(rising_list_a[0])\n",
    "rising_list[1] = process_results(rising_list_a[1])\n",
    "\n",
    " # month_rule_list_a를 처리하면서 추가 데이터 처리를 포함\n",
    "month_rule_list = []  # 초기화가 필요할 수 있습니다.\n",
    "for result in month_rule_list_a:\n",
    "    if not all(value is None for value in result) and result[0] is not None:\n",
    "        column_names = result[0].columns\n",
    "        data_values_list = result[0][column_names].values.flatten()  # 데이터를 1D 배열로 변환\n",
    "        additional_data = {\n",
    "            'Indicator': data_values_list.tolist(),  # numpy 배열을 리스트로 변환\n",
    "            'RisingMonth': result[3],\n",
    "            '유형': '월별규칙성'  # 모든 결과에 대해 '유형'을 '월별규칙성'으로 설정\n",
    "        }\n",
    "        month_rule_list += process_results_month([result], additional_data=additional_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_10392\\1937458494.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 리스트와 유형을 매핑\n",
    "lists_and_types = [\n",
    "    (select_list[0], '일별급상승'),\n",
    "    (select_list[1], '주별급상승'),\n",
    "    (select_list[2], '월별급상승'),\n",
    "    (rising_list[0], '주별지속상승'),\n",
    "    (rising_list[1], '월별지속상승'),\n",
    "    (month_rule_list, '월별규칙성')\n",
    "]\n",
    "\n",
    "\n",
    "# 모든 리스트를 처리하고 하나의 데이터프레임으로 병합\n",
    "processed_dfs = [utils.process_and_concat(df_list, label) for df_list, label in lists_and_types]\n",
    "\n",
    "\n",
    "# 비어 있지 않은 DataFrame들만 병합\n",
    "graph_result = pd.concat([df for df in processed_dfs if not df.empty]).reset_index(drop=True)\n",
    "\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "# 불필요한 컬럼 삭제 및 '주간지속상승'을 '주별지속상승'으로 수정\n",
    "\n",
    "\n",
    "graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n",
    "graph_result['RisingMonth'] = graph_result['RisingMonth'].replace({None: 0})\n",
    "graph_result['RisingMonth'] = graph_result['RisingMonth'].fillna(' ')\n",
    "# # 정렬\n",
    "graph_result.sort_values(by=['연관검색어', '유형', '검색일자'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "# # 최종 결과 출력 ( 그래프 함수  )\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "print(1)\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "flags_and_lists = [\n",
    "    (\"일별 급상승\", select_list[0]),\n",
    "    (\"주별 급상승\", select_list[1]),\n",
    "    (\"주별 지속상승\", rising_list[0]),\n",
    "    (\"월별 급상승\", select_list[2]),\n",
    "    (\"월별 지속상승\", rising_list[1]),\n",
    "    (\"월별 규칙성\", month_rule_list),\n",
    "]\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # data_list가 리스트인지 확인\n",
    "    if not isinstance(data_list, list):\n",
    "        print(f\"{flag_name}: data_list가 리스트가 아닙니다.\")\n",
    "        continue\n",
    "    \n",
    "    # data_list 내의 각 요소가 DataFrame인지, '연관검색어' 컬럼이 있는지 확인\n",
    "    for idx, df in enumerate(data_list):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"{flag_name}: 인덱스 {idx}에 DataFrame이 아닌 요소가 있습니다.\")\n",
    "        elif \"연관검색어\" not in df.columns:\n",
    "            print(f\"{flag_name}: 인덱스 {idx}의 DataFrame에 '연관검색어' 컬럼이 없습니다.\")\n",
    "\n",
    "# utils.update_keywords_flag 함수를 호출하기 전에 각 data_list의 유효성 검사\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # 데이터 프레임으로 구성된 리스트만 유지\n",
    "    valid_data_list = [df for df in data_list if isinstance(df, pd.DataFrame) and \"연관검색어\" in df.columns]\n",
    "    \n",
    "    # 유효한 데이터 리스트만을 사용하여 키워드 플래그 업데이트\n",
    "    utils.update_keywords_flag(collected_keywords_data, valid_data_list, flag_name)\n",
    " # process_data : 지정된 조건에 따라 데이터를 필터링하고, 추가 처리를 통해 최종 데이터프레임을 반환하는 함수.\n",
    "def safe_process_data(process_function, data, category1, category2, selection):\n",
    "    \"\"\"\n",
    "    process_function: 데이터 처리 함수 (예: utils.process_data)\n",
    "    data: 처리할 데이터프레임\n",
    "    category1, category2: 데이터 처리 함수에 전달될 카테고리 인자\n",
    "    selection: 데이터 처리 함수에 전달될 선택 리스트 또는 기타 인자\n",
    "    \n",
    "    반환값: 처리된 데이터프레임 또는 빈 데이터프레임\n",
    "    \"\"\"\n",
    "    if data is not None and not data.empty:\n",
    "        try:\n",
    "            return process_function(data, category1, category2, selection)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data: {e}\")\n",
    "            # 처리 중 오류가 발생한 경우 빈 데이터프레임 반환\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No data available.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "info_result_daily_select = safe_process_data(utils.process_data, collected_keywords_data, '일별 급상승', '일별 급상승', select_list[0])\n",
    "\n",
    "info_result_weekly_select = utils.process_data(collected_keywords_data, '주별 급상승', '주별 급상승', select_list[1])\n",
    "info_result_monthly_select = utils.process_data(collected_keywords_data, '월별 급상승', '월별 급상승', select_list[2]) \n",
    "\n",
    "info_result_weekly_continuous = utils.process_data(collected_keywords_data, '주별 지속상승', '주별 지속상승', rising_list[0])\n",
    "\n",
    "info_result_monthly_continuous = utils.process_data(collected_keywords_data, '월별 지속상승', '월별 지속상승', rising_list[1])\n",
    "\n",
    "info_result_monthly_pattern = utils.process_data(collected_keywords_data, '월별 규칙성', '월별 규칙성', month_rule_list)\n",
    "\n",
    "info_result_final = pd.concat([info_result_daily_select,info_result_weekly_select, info_result_monthly_select,\\\n",
    "                               info_result_weekly_continuous, info_result_monthly_continuous,\\\n",
    "                                  info_result_monthly_pattern]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구글/ 네이버 한꺼번에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 뉴스링크,제목 수집 (네이버)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def collect_google_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"google_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            rising_keywords_results = pickle.load(file)\n",
    "    else:\n",
    "        rising_keywords_results = await collect_rising_keywords(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(rising_keywords_results, file)\n",
    "    \n",
    "    return rising_keywords_results\n",
    "\n",
    "async def collect_news_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"news_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            news_data = pickle.load(file)\n",
    "    else:\n",
    "        news_data = await main_news(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(news_data, file)\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# 메인 비동기 실행 함수\n",
    "async def main(target_keywords):\n",
    "    google_keywords_results, news_keywords_results = await asyncio.gather(\n",
    "        collect_google_keywords(target_keywords),\n",
    "        collect_news_keywords(target_keywords)\n",
    "    )\n",
    "    \n",
    "    return google_keywords_results, news_keywords_results\n",
    "\n",
    "\n",
    "\n",
    "target_keywords = list(set(info_result_final['연관키워드']))\n",
    "rising_keywords_results,news_data=asyncio.run(main(target_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/target_keywords/240329\\target_keywords.txt 파일이 이미 존재합니다. 작업을 건너뜁니다.\n",
      "./data/target_keywords/240329/keyword_activity_rates.csv 파일이 이미 존재합니다. 작업을 건너뜁니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################\n",
    "#활동성 분석\n",
    "################################\n",
    "import subprocess\n",
    "today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "directory_path = f\"./data/target_keywords/{today_date}\"\n",
    "file_path = os.path.join(directory_path, \"target_keywords.txt\")\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    # 디렉토리가 존재하지 않는 경우, 디렉토리 생성\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# 파일이 존재하는지 확인\n",
    "if not os.path.exists(file_path):\n",
    "    # 키워드를 파일에 작성\n",
    "    with open(file_path, 'w') as file:\n",
    "        for keyword in target_keywords:\n",
    "            file.write(\"%s\\n\" % keyword)\n",
    "    result = f\"{file_path}에 키워드 저장됨\"\n",
    "else:\n",
    "    result = f\"{file_path} 파일이 이미 존재합니다. 작업을 건너뜁니다.\"\n",
    "print(result)\n",
    "\n",
    "# 파일이 존재하지 않는 경우, blog_data_collector.py 스크립트를 실행\n",
    "file_path = f\"./data/target_keywords/{today_date}/keyword_activity_rates.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    command = f\"python  models/naver/blog.py\"\n",
    "    process = subprocess.run(command, shell=True, check=True)\n",
    "    result = f\"{file_path}에 작업 결과가 저장될 것입니다.\"\n",
    "else:\n",
    "    result = f\"{file_path} 파일이 이미 존재합니다. 작업을 건너뜁니다.\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_keyword_activity_rates=formatting.merge_data(news_data,directory_path)\n",
    "final_merged_df_copy=formatting.merge_result(collected_keywords_dat_copy,info_result_final,merged_keyword_activity_rates,rising_keywords_results)\n",
    "info_data,combined_df=formatting.reults_formatted(info_result_final,final_merged_df_copy,graph_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구분자 ,\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n",
    "save_path=f\"./data/result_out/{formatted_today}/\"\n",
    "combined_df.to_csv(f'{save_path}/graph_{formatted_today}_in.csv', encoding='utf-8-sig', index=False, header=False)\n",
    "info_data.to_csv(f'{save_path}/info_{formatted_today}_in.csv', encoding='utf-8-sig', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = utils.make_csv(info_data)\n",
    "\n",
    "# 현재 날짜를 'yyMMdd' 형식으로 포맷팅\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n",
    "\n",
    "# 저장할 경로\n",
    "save_path = f'./data/result_out/{formatted_today}'\n",
    "\n",
    "# 해당 경로가 존재하지 않으면 생성\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# CSV 파일 저장\n",
    "result_csv.to_csv(f'{save_path}/info_{formatted_today}.csv', encoding='utf-8-sig', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_graph = utils.make_csv(combined_df)\n",
    "\n",
    "\n",
    "# 현재 날짜를 'yyMMdd' 형식으로 포맷팅\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n",
    "\n",
    "# 저장할 경로\n",
    "save_path = f'./data/result_out/{formatted_today}'\n",
    "\n",
    "# 해당 경로가 존재하지 않으면 생성\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# CSV 파일 저장\n",
    "result_graph.to_csv(f'{save_path}/graph_{formatted_today}.csv', encoding='utf-8-sig', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모니터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted latest date before today: 240328\n"
     ]
    }
   ],
   "source": [
    "result_out_path = './data/result_out/'\n",
    "formatted_latest_date = utils.find_latest_date_before_today(result_out_path)\n",
    "if formatted_latest_date:\n",
    "    print(\"Formatted latest date before today:\", formatted_latest_date)\n",
    "else:\n",
    "    print(\"No valid date folder found before today.\")\n",
    "\n",
    "info_data_today = info_data.copy()\n",
    "file_path = f'./data/result_out/{formatted_latest_date}/info_{formatted_latest_date}.csv'\n",
    "if 'info_data_yesterday' not in locals():\n",
    "    info_data_beforeday = pd.read_csv(file_path, sep='\\|\\|\\|', engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유형별 영문&한글 이름\n",
    "trend_type = dict(zip(['daily_up_trend', 'weekly_up_trend', 'weekly_stay_trend', 'monthly_up_trend', 'monthly_stay_trend', 'monthly_rule_trend'],\n",
    "                        ['일별 급상승', '주별 급상승', '주별 지속상승', '월별 급상승', '월별 지속상승', '월별 규칙성']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일별 급상승</th>\n",
       "      <th>주별 급상승</th>\n",
       "      <th>주별 지속상승</th>\n",
       "      <th>월별 급상승</th>\n",
       "      <th>월별 지속상승</th>\n",
       "      <th>월별 규칙성</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240328 검색어 수</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>195</td>\n",
       "      <td>196</td>\n",
       "      <td>94</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240329 검색어 수</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>186</td>\n",
       "      <td>193</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>검색어 증감</th>\n",
       "      <td>0</td>\n",
       "      <td>-8</td>\n",
       "      <td>-9</td>\n",
       "      <td>-3</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>검색어 증감(%)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.48</td>\n",
       "      <td>-4.62</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>-4.26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>새로운 검색어</th>\n",
       "      <td>'상장주식', '전자화폐', '상속전문변호사', '삼천당제약주가', '중고노트북매입...</td>\n",
       "      <td>'해남부동산', '삼성전자주가', '산티아고순례길', '재택근무직업', '인텔주가'...</td>\n",
       "      <td>'신용카드캐시백', '부가세', '부업추천', '가계부', '나스닥ETF', '자동...</td>\n",
       "      <td>'KODEX미국S&amp;P500TR', '그림전시', '이더리움시세', '상장폐지주식',...</td>\n",
       "      <td>'보증금대출', '월세지원금', '배당주', '일본주식', '손해보험'</td>\n",
       "      <td>'법인등기', '당뇨보험', '평생교육원', '보험사', '공모전', '부동산매매'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>빠진 검색어</th>\n",
       "      <td>'스팩주', '쓰레스홀드코인', '레이븐코인', '3D프린터기', '코인상장', '...</td>\n",
       "      <td>'케이비손해보험', '종합보험', '삼성증권ISA', '실손보험비교', '자동차보험...</td>\n",
       "      <td>'ETF수익률', '베트남환율', '레이븐코인', '오스코텍주가', '건강보험', ...</td>\n",
       "      <td>'코스모스코인', '해외선물수수료', '밈코인', '금팔때시세', '실비보험비교사이...</td>\n",
       "      <td>'관세', '해외선물수수료', '바이오ETF', '조세특례제한법', '프랜차이즈박람...</td>\n",
       "      <td>'요식업', '강제집행', '벤처기업인증', '상속포기신청', '경매', '솔루션'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>주요 검색어</th>\n",
       "      <td>배당금계산기</td>\n",
       "      <td>삼성전자우</td>\n",
       "      <td>대여금반환청구소송</td>\n",
       "      <td>금값시세</td>\n",
       "      <td>종합보험</td>\n",
       "      <td>연말정산간소화</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         일별 급상승  \\\n",
       "240328 검색어 수                                                  7   \n",
       "240329 검색어 수                                                  7   \n",
       "검색어 증감                                                        0   \n",
       "검색어 증감(%)                                                   0.0   \n",
       "새로운 검색어       '상장주식', '전자화폐', '상속전문변호사', '삼천당제약주가', '중고노트북매입...   \n",
       "빠진 검색어        '스팩주', '쓰레스홀드코인', '레이븐코인', '3D프린터기', '코인상장', '...   \n",
       "주요 검색어                                                   배당금계산기   \n",
       "\n",
       "                                                         주별 급상승  \\\n",
       "240328 검색어 수                                                107   \n",
       "240329 검색어 수                                                 99   \n",
       "검색어 증감                                                       -8   \n",
       "검색어 증감(%)                                                 -7.48   \n",
       "새로운 검색어       '해남부동산', '삼성전자주가', '산티아고순례길', '재택근무직업', '인텔주가'...   \n",
       "빠진 검색어        '케이비손해보험', '종합보험', '삼성증권ISA', '실손보험비교', '자동차보험...   \n",
       "주요 검색어                                                    삼성전자우   \n",
       "\n",
       "                                                        주별 지속상승  \\\n",
       "240328 검색어 수                                                195   \n",
       "240329 검색어 수                                                186   \n",
       "검색어 증감                                                       -9   \n",
       "검색어 증감(%)                                                 -4.62   \n",
       "새로운 검색어       '신용카드캐시백', '부가세', '부업추천', '가계부', '나스닥ETF', '자동...   \n",
       "빠진 검색어        'ETF수익률', '베트남환율', '레이븐코인', '오스코텍주가', '건강보험', ...   \n",
       "주요 검색어                                                대여금반환청구소송   \n",
       "\n",
       "                                                         월별 급상승  \\\n",
       "240328 검색어 수                                                196   \n",
       "240329 검색어 수                                                193   \n",
       "검색어 증감                                                       -3   \n",
       "검색어 증감(%)                                                 -1.53   \n",
       "새로운 검색어       'KODEX미국S&P500TR', '그림전시', '이더리움시세', '상장폐지주식',...   \n",
       "빠진 검색어        '코스모스코인', '해외선물수수료', '밈코인', '금팔때시세', '실비보험비교사이...   \n",
       "주요 검색어                                                     금값시세   \n",
       "\n",
       "                                                        월별 지속상승  \\\n",
       "240328 검색어 수                                                 94   \n",
       "240329 검색어 수                                                 90   \n",
       "검색어 증감                                                       -4   \n",
       "검색어 증감(%)                                                 -4.26   \n",
       "새로운 검색어                 '보증금대출', '월세지원금', '배당주', '일본주식', '손해보험'   \n",
       "빠진 검색어        '관세', '해외선물수수료', '바이오ETF', '조세특례제한법', '프랜차이즈박람...   \n",
       "주요 검색어                                                     종합보험   \n",
       "\n",
       "                                                         월별 규칙성  \n",
       "240328 검색어 수                                                 88  \n",
       "240329 검색어 수                                                 88  \n",
       "검색어 증감                                                        0  \n",
       "검색어 증감(%)                                                   0.0  \n",
       "새로운 검색어       '법인등기', '당뇨보험', '평생교육원', '보험사', '공모전', '부동산매매'...  \n",
       "빠진 검색어        '요식업', '강제집행', '벤처기업인증', '상속포기신청', '경매', '솔루션'...  \n",
       "주요 검색어                                                  연말정산간소화  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어제 분석 결과\n",
    "yest_table, yest_word = utils.generate_result_list(info_data_today, trend_type)\n",
    "\n",
    "# 금일 분석 결과\n",
    "recent_table, recent_word = utils.generate_result_list(info_data_beforeday, trend_type)\n",
    "# 모니터링을 위한 새로운 변수명 사용\n",
    "index_names = [\n",
    "    f'{formatted_latest_date} 검색어 수', \n",
    "    f'{formatted_today} 검색어 수', \n",
    "    '검색어 증감', \n",
    "    '검색어 증감(%)', \n",
    "    '새로운 검색어', \n",
    "    '빠진 검색어', \n",
    "    '주요 검색어'\n",
    "]\n",
    "\n",
    "# 모니터링 결과 파일 저장\n",
    "output_file_path = f'./data/result_out/{day}/monitor_in_{day}.csv'\n",
    "diff_table=utils.make_diff(trend_type ,index_names, recent_table,recent_word,yest_table, yest_word)\n",
    "diff_table.to_csv(output_file_path, encoding='utf-8-sig')\n",
    "\n",
    "# 출력 데이터프레임 조정 및 CSV 파일로 저장\n",
    "adjusted_diff_table = diff_table.reset_index()\n",
    "adjusted_csv = utils.make_csv(adjusted_diff_table)\n",
    "adjusted_csv_path = f'./data/result_out/{day}/monitor_out_{day}.csv'\n",
    "adjusted_csv.to_csv(adjusted_csv_path, encoding='utf-8-sig', index=False, header=False)\n",
    "\n",
    "# 결과 데이터프레임 반환\n",
    "diff_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_folders_except_recent(base_path, folder_name, keep_recent=3):\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "    subfolders = sorted([f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))])\n",
    "\n",
    "    # 최근 3개를 제외한 모든 폴더를 삭제 대상으로 설정\n",
    "    folders_to_delete = subfolders[:-keep_recent]\n",
    "\n",
    "    for folder in folders_to_delete:\n",
    "        full_path = os.path.join(folder_path, folder)\n",
    "        shutil.rmtree(full_path)\n",
    "        print(f\"Deleted {full_path}\")\n",
    "\n",
    "# 기본 'data' 폴더 경로. 이 경로를 실제 환경에 맞게 조정하세요.\n",
    "base_path = './data/'\n",
    "\n",
    "# 각 하위 폴더 이름\n",
    "subfolder_names = ['result_out', 'rl_srch', 'target_keywords', 'trend_data']\n",
    "\n",
    "# 각 하위 폴더에 대해 삭제 로직 실행\n",
    "for folder_name in subfolder_names:\n",
    "    delete_folders_except_recent(base_path, folder_name)\n",
    "\n",
    "print(\"Deletion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info_240329_in.csv 파일이 ./data.zip에 성공적으로 추가되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 압축할 파일의 경로\n",
    "file_to_zip_path = f'./data/result_out/{formatted_today}/info_{formatted_today}_in.csv'\n",
    "\n",
    "# 생성할 ZIP 파일의 경로 및 이름 (확장자 포함)\n",
    "archive_path = './data.zip'\n",
    "\n",
    "# 새 ZIP 파일 생성 및 특정 파일 추가\n",
    "with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # 파일이 실제로 존재하는지 확인\n",
    "    if os.path.exists(file_to_zip_path):\n",
    "        # ZIP 파일에 파일 추가\n",
    "        zipf.write(file_to_zip_path, os.path.basename(file_to_zip_path))\n",
    "        print(f\"{os.path.basename(file_to_zip_path)} 파일이 {archive_path}에 성공적으로 추가되었습니다.\")\n",
    "    else:\n",
    "        print(f\"{file_to_zip_path} 파일을 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# 파일 경로 지정\n",
    "# 파일 경로 지정\n",
    "file_path = f'./data/result_out/{formatted_today}/'\n",
    "file_names = [\n",
    "    f'graph_{formatted_today}.csv',\n",
    "    f\"graph_{formatted_today}_in.csv\",\n",
    "    f\"info_{formatted_today}.csv\",\n",
    "    f\"info_{formatted_today}_in.csv\",\n",
    "    f\"monitor_out_{formatted_today}.csv\",\n",
    "    f\"monitor_in_{formatted_today}.csv\"\n",
    "]\n",
    "\n",
    "# 파일 다운로드\n",
    "for file_name in file_names:\n",
    "    try:\n",
    "        files.download(file_path + file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {file_name}: {e}\")\n",
    "data폴더 다운\n",
    "files.download(\"data.zip.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
