{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  버전 설치\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dtw import *\n",
    "import pickle\n",
    "from pytrends.request import TrendReq\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "from pytrends.request import TrendReq\n",
    "import nest_asyncio\n",
    "from models.naver.blog import activity_rate\n",
    "from api_set import APIClient\n",
    "import utils\n",
    "import models.crawling.trend as trend \n",
    "from models.crawling.collect_keywords import collect_keywords\n",
    "from models.crawling.google_trend import collect_rising_keywords\n",
    "from models.naver.news import main_news \n",
    "from models.crawling.select_keyword import select_keyword, rising_keyword_analysis, monthly_rule\n",
    "from models.anaysis import execute_analysis , process_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. API설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API 설정\n",
    "BASE_URL = utils.get_secret(\"BASE_URL\")\n",
    "CUSTOMER_ID = utils.get_secret(\"CUSTOMER_ID\")\n",
    "API_KEY = utils.get_secret(\"API_KEY\")\n",
    "SECRET_KEY = utils.get_secret(\"SECRET_KEY\")\n",
    "URI = utils.get_secret(\"URI\")\n",
    "METHOD = utils.get_secret(\"METHOD\")\n",
    "# API 클라이언트 인스턴스 생성\n",
    "api_client = APIClient(BASE_URL, CUSTOMER_ID, API_KEY, SECRET_KEY,URI,METHOD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 연관검색어 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키 로드\n",
    "keywords_data = utils.load_keywords('main_keyword.json')\n",
    "\n",
    "# 오늘의 날짜 가져오기\n",
    "formatted_today, day = utils.get_today_date()\n",
    "\n",
    "\n",
    "utils.make_directory('./data')\n",
    "utils.make_directory('./data/rl_srch')\n",
    "utils.make_directory(f'./data/rl_srch/{day}')  # 키워드별 연관검색어 리스트 저장\n",
    "\n",
    "# 검색어 리스트와 결과 저장 경로 설정\n",
    "srch_keyword = ['keyword_final']  \n",
    "save_path = './data/rl_srch/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main(srch_keyword, day):\n",
    "    # 오늘 날짜로 폴더 경로 생성\n",
    "    folder_path = './data/rl_srch/' + datetime.now().strftime('%y%m%d')\n",
    "    file_path = f\"{folder_path}/collected_keywords.csv\"\n",
    "    \n",
    "    # 폴더가 존재하는지 확인\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # 파일이 존재하는지 확인\n",
    "    if os.path.isfile(file_path):\n",
    "        # 파일이 존재하면, 데이터를 읽어옵니다.\n",
    "        collected_keywords_data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # 파일이 없으면, collect_keywords 함수를 호출해서 데이터를 수집합니다.\n",
    "        collected_keywords_data = await collect_keywords(srch_keyword, day)\n",
    "        # 결과를 CSV로 저장\n",
    "        collected_keywords_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    return collected_keywords_data\n",
    "collected_keywords_data=asyncio.run(main(srch_keyword, day))\n",
    "\n",
    "collected_keywords_dat_copy=asyncio.run(main(srch_keyword, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 중복검색어컬럼 생성, 중복제거하고 각 키워드별로 50개씩 집계하는 로직 \n",
    "\n",
    "# 1. collected_keywords_data의 복사본 생성\n",
    "temp_df = collected_keywords_data.copy()\n",
    "\n",
    "# 2. 새로운 컬럼 '중복검색어' 추가 (초기값으로 빈 문자열 할당)\n",
    "temp_df['중복검색어'] = ''\n",
    "\n",
    "# 3. 연관키워드별로 해당하는 모든 검색어를 찾는 딕셔너리 생성\n",
    "keywords_dict = {}\n",
    "for index, row in temp_df.iterrows():\n",
    "    associated_keyword = row['연관키워드']\n",
    "    search_keyword = row['검색어']\n",
    "    if associated_keyword in keywords_dict:\n",
    "        # 이미 리스트에 있는 경우 중복을 피하기 위해 추가하지 않음\n",
    "        if search_keyword not in keywords_dict[associated_keyword]:\n",
    "            keywords_dict[associated_keyword].append(search_keyword)\n",
    "    else:\n",
    "        # 새로운 키워드인 경우 리스트 초기화\n",
    "        keywords_dict[associated_keyword] = [search_keyword]\n",
    "\n",
    "# 4. '중복검색어' 컬럼을 채워 넣음\n",
    "for index, row in temp_df.iterrows():\n",
    "    associated_keyword = row['연관키워드']\n",
    "    # 연관키워드에 해당하는 모든 검색어를 '중복검색어' 컬럼에 할당\n",
    "    temp_df.at[index, '중복검색어'] = ','.join(keywords_dict[associated_keyword])\n",
    "df_list_test = []\n",
    "already_selected = set()\n",
    "for _, group in temp_df.groupby('검색어'):\n",
    "    selected_rows = []  # Collect rows to append\n",
    "    for index, row in group.iterrows():\n",
    "        if row['연관키워드'] not in already_selected:\n",
    "            selected_rows.append(row)\n",
    "            already_selected.add(row['연관키워드'])\n",
    "        if len(selected_rows) >= 50:\n",
    "            break\n",
    "    # Append all selected rows at once to improve performance\n",
    "    filtered_group = pd.DataFrame(selected_rows)\n",
    "    df_list_test.append(filtered_group)\n",
    "collected_keywords_data = pd.concat(df_list_test, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_list = [group for _, group in collected_keywords_data.groupby('검색어')]\n",
    "# collected_keywords_data = utils.merge_and_mark_duplicates_limited(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_keywords_data= utils.add_client_info(collected_keywords_data)\n",
    "new_columns = ['일별급상승', '주별급상승', '월별급상승', '주별지속상승', '월별지속상승', '월별규칙성']\n",
    "\n",
    "for column in new_columns:\n",
    "    collected_keywords_data[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def groupped_df(name,collected_keywords_data):\n",
    "    grouped = collected_keywords_data.groupby(name)\n",
    "    df_list = [group for _, group in grouped]\n",
    "    return df_list\n",
    "df_list=groupped_df('id',collected_keywords_data)\n",
    "n=len(df_list)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터를 로드하거나 크롤링하여 반환하는 비동기 함수\n",
    "async def load_or_crawl_data(df_list, clients):\n",
    "    today_date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory = f\"./data/trend_data/{today_date_str}\"\n",
    "    save_path = f\"{directory}/data_{today_date_str}.pkl\"\n",
    "    \n",
    "    # 파일이 존재하면 데이터 로드\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    else:\n",
    "        # 파일이 없으면 비동기 크롤링 시작\n",
    "        results = await run_all(df_list, clients)\n",
    "        # 결과 데이터 저장\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 비동기 크롤링 함수\n",
    "async def trend_main(df, clients):\n",
    "    params = {\n",
    "        \"search_keywords\": list(df['연관키워드']),\n",
    "        \"id\": df['id'].iloc[0],\n",
    "        \"pw\": df['pw'].iloc[0],\n",
    "        \"api_url\": \"https://openapi.naver.com/v1/datalab/search\",\n",
    "        \"name\": '연관검색어'\n",
    "    }\n",
    "    api_url = \"https://openapi.naver.com/v1/datalab/search\"\n",
    "    \n",
    "    # trend_maincode 함수 실행\n",
    "    results = await trend.trend_maincode(params, clients, api_url)\n",
    "    return results\n",
    "\n",
    "async def run_all(df_list, clients):\n",
    "    tasks = [trend_main(df, clients) for df in df_list]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "clients = utils.get_secret(\"clients\")  # clients 정보를 로드\n",
    "\n",
    "# 이벤트 루프 실행 및 데이터 로드 또는 크롤링\n",
    "trend_main_data = asyncio.run(load_or_crawl_data(df_list, clients))\n",
    "results = trend_main_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직렬로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start_time = time.time()\n",
    "# select_periods = ['daily', 'weekly', 'month']\n",
    "# rising_periods=['weekly', 'month']\n",
    "\n",
    "# formatted_today, today_date = utils.get_today_date()\n",
    "# month_rule_list=[]\n",
    "# select_list=[[],[],[]]\n",
    "\n",
    "# rising_list=[[],[]]\n",
    "# rising_month_list=[]\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# # 일별, 주별, 월별 키워드 선택 실행\n",
    "# for period in select_periods:\n",
    "#     for keyword_df_group in results:\n",
    "#         for keyword_df in keyword_df_group:\n",
    "#             selected_tmp, selected_graph, selected_info = select_keyword(keyword_df, today_date, period)\n",
    "#             if selected_graph is not None:\n",
    "#                 # 데이터프레임의 열 이름을 출력합니다.\n",
    "#                 selected_graph['InfoData'] = selected_info\n",
    "#                 select_list[i].append(selected_graph)\n",
    "#             else:\n",
    "#                 pass\n",
    "#     i += 1\n",
    "# # 월별, 주별, 일별 키워드 분석 실행\n",
    "\n",
    "#     # 각 분석 기간에 대해 결과 집합을 순회합니다.\n",
    "# for keyword_group in results:\n",
    "#     # 키워드 그룹의 각 키워드 데이터프레임에 대해 순회합니다.\n",
    "#     for keyword_data in keyword_group:\n",
    "#         # 월별 규칙을 적용하여 결과를 가져옵니다.\n",
    "#         monthly_data, monthly_chart, similarity_rate, rising_months = monthly_rule(keyword_data, today_date, 'month')\n",
    "        \n",
    "#         if monthly_data is not None:\n",
    "#             # 결과 데이터프레임의 열 이름을 가져옵니다.\n",
    "#             column_names = monthly_data.columns\n",
    "#             rising_month_list.append([rising_months,column_names[0]])\n",
    "#             # 결과 데이터프레임에서 값 리스트를 추출합니다.\n",
    "#             data_values_list = monthly_data[column_names].values\n",
    "#             # 월별 차트에 데이터 값을 추가합니다.\n",
    "#             monthly_chart['Indicator'] = data_values_list\n",
    "#             monthly_chart['InfoData'] = similarity_rate\n",
    "#             # 상승 월 정보를 추가합니다. 상승 월이 없는 경우 0으로 설정합니다.\n",
    "#             monthly_chart['RisingMonth'] = 0\n",
    "            \n",
    "#             # 최종 결과 리스트에 수정된 월별 차트를 추가합니다.\n",
    "#             month_rule_list.append(monthly_chart)\n",
    "                \n",
    "# # 주별, 월별 상승 키워드 분석 실행\n",
    "# rising_analysis_periods = ['weekly', 'month']\n",
    "# i=0\n",
    "# for period in rising_analysis_periods:\n",
    "#     for keyword_df_group in results:\n",
    "#         for keyword_df in keyword_df_group:\n",
    "#             rising_tmp, rising_graph, rising_info = rising_keyword_analysis(keyword_df, today_date, period)\n",
    "#             if rising_tmp is not None:\n",
    "#                 column_names=rising_tmp.columns\n",
    "#                 data_values_list = rising_tmp[column_names].values\n",
    "#                 rising_graph['Indicator'] = data_values_list\n",
    "#                 rising_graph['InfoData'] = rising_info\n",
    "\n",
    "#                 rising_list[i].append(rising_graph)\n",
    "#     i=i+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"Analysis completed in {end_time - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전역 변수로 리스트 초기화\n",
    "month_rule_list_a = []\n",
    "rising_list_a = [[], []]  # 주별 상승, 월별 상승\n",
    "select_list_a = [[], [], []]  # 일별 선택, 주별 선택, 월별 선택\n",
    "execute_analysis(results,month_rule_list_a,rising_list_a,select_list_a)\n",
    "\n",
    "\n",
    "month_rule_list=[]\n",
    "select_list=[[],[],[]]\n",
    "\n",
    "rising_list=[[],[]]\n",
    "rising_month_list=[]\n",
    "\n",
    "\n",
    "# 각 리스트를 처리\n",
    "select_list[0] = process_results(select_list_a[0])\n",
    "select_list[1] = process_results(select_list_a[1])\n",
    "select_list[2] = process_results(select_list_a[2])\n",
    "\n",
    "rising_list[0] = process_results(rising_list_a[0])\n",
    "rising_list[1] = process_results(rising_list_a[1])\n",
    "\n",
    "# month_rule_list_a를 처리하면서 추가 데이터 처리를 포함\n",
    "for result in month_rule_list_a:\n",
    "    if not all(value is None for value in result) and result[0] is not None:\n",
    "        column_names = result[0].columns\n",
    "        data_values_list = result[0][column_names].values\n",
    "        additional_data = {\n",
    "            'Indicator': data_values_list,\n",
    "            'RisingMonth': 0,\n",
    "            '유형': '월별규칙성'  # 모든 결과에 대해 '유형'을 '월별규칙성'으로 설정\n",
    "        }\n",
    "        month_rule_list += process_results([result], additional_data=additional_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_58764\\1875387929.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 리스트와 유형을 매핑\n",
    "lists_and_types = [\n",
    "    (select_list[0], '일별급상승'),\n",
    "    (select_list[1], '주별급상승'),\n",
    "    (select_list[2], '월별급상승'),\n",
    "    (rising_list[0], '주별지속상승'),\n",
    "    (rising_list[1], '월별지속상승'),\n",
    "    (month_rule_list, '월별규칙성')\n",
    "]\n",
    "\n",
    "\n",
    "# 모든 리스트를 처리하고 하나의 데이터프레임으로 병합\n",
    "processed_dfs = [utils.process_and_concat(df_list, label) for df_list, label in lists_and_types]\n",
    "\n",
    "# 비어 있지 않은 DataFrame들만 병합\n",
    "graph_result = pd.concat([df for df in processed_dfs if not df.empty]).reset_index(drop=True)\n",
    "\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "# 불필요한 컬럼 삭제 및 '주간지속상승'을 '주별지속상승'으로 수정\n",
    "\n",
    "graph_result = graph_result.drop(columns=['InfoData'])\n",
    "graph_result['유형'].replace({'주간지속상승': '주별지속상승'}, inplace=True)\n",
    "\n",
    "# 정렬\n",
    "graph_result.sort_values(by=['연관검색어', '유형', '검색일자'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "# 최종 결과 출력\n",
    "graph_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_and_lists = [\n",
    "    (\"일별 급상승\", select_list[0]),\n",
    "    (\"주별 급상승\", select_list[1]),\n",
    "    (\"주별 지속상승\", rising_list[0]),\n",
    "    (\"월별 급상승\", select_list[2]),\n",
    "    (\"월별 지속상승\", rising_list[1]),\n",
    "    (\"월별 규칙성\", month_rule_list),\n",
    "]\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # data_list가 리스트인지 확인\n",
    "    if not isinstance(data_list, list):\n",
    "        print(f\"{flag_name}: data_list가 리스트가 아닙니다.\")\n",
    "        continue\n",
    "    \n",
    "    # data_list 내의 각 요소가 DataFrame인지, '연관검색어' 컬럼이 있는지 확인\n",
    "    for idx, df in enumerate(data_list):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"{flag_name}: 인덱스 {idx}에 DataFrame이 아닌 요소가 있습니다.\")\n",
    "        elif \"연관검색어\" not in df.columns:\n",
    "            print(f\"{flag_name}: 인덱스 {idx}의 DataFrame에 '연관검색어' 컬럼이 없습니다.\")\n",
    "\n",
    "# utils.update_keywords_flag 함수를 호출하기 전에 각 data_list의 유효성 검사\n",
    "for flag_name, data_list in flags_and_lists:\n",
    "    # 데이터 프레임으로 구성된 리스트만 유지\n",
    "    valid_data_list = [df for df in data_list if isinstance(df, pd.DataFrame) and \"연관검색어\" in df.columns]\n",
    "    \n",
    "    # 유효한 데이터 리스트만을 사용하여 키워드 플래그 업데이트\n",
    "    utils.update_keywords_flag(collected_keywords_data, valid_data_list, flag_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # process_data : 지정된 조건에 따라 데이터를 필터링하고, 추가 처리를 통해 최종 데이터프레임을 반환하는 함수.\n",
    "def safe_process_data(process_function, data, category1, category2, selection):\n",
    "    \"\"\"\n",
    "    process_function: 데이터 처리 함수 (예: utils.process_data)\n",
    "    data: 처리할 데이터프레임\n",
    "    category1, category2: 데이터 처리 함수에 전달될 카테고리 인자\n",
    "    selection: 데이터 처리 함수에 전달될 선택 리스트 또는 기타 인자\n",
    "    \n",
    "    반환값: 처리된 데이터프레임 또는 빈 데이터프레임\n",
    "    \"\"\"\n",
    "    if data is not None and not data.empty:\n",
    "        try:\n",
    "            return process_function(data, category1, category2, selection)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data: {e}\")\n",
    "            # 처리 중 오류가 발생한 경우 빈 데이터프레임 반환\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No data available.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "info_result_daily_select = safe_process_data(utils.process_data, collected_keywords_data, '일별 급상승', '일별 급상승', select_list[0])\n",
    "info_result_weekly_select = utils.process_data(collected_keywords_data, '주별 급상승', '주별 급상승', select_list[1])\n",
    "info_result_monthly_select = utils.process_data(collected_keywords_data, '월별 급상승', '월별 급상승', select_list[2]) \n",
    "\n",
    "info_result_weekly_continuous = utils.process_data(collected_keywords_data, '주별 지속상승', '주별 지속상승', rising_list[0])\n",
    "\n",
    "info_result_monthly_continuous = utils.process_data(collected_keywords_data, '월별 지속상승', '월별 지속상승', rising_list[1])\n",
    "\n",
    "info_result_monthly_pattern = utils.process_data(collected_keywords_data, '월별 규칙성', '월별 규칙성', month_rule_list)\n",
    "\n",
    "info_result_final = pd.concat([info_result_daily_select,info_result_weekly_select, info_result_monthly_select,\\\n",
    "                               info_result_weekly_continuous, info_result_monthly_continuous,\\\n",
    "                                  info_result_monthly_pattern]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구글/ 네이버 한꺼번에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 뉴스링크,제목 수집 (네이버)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['증시현황', '돌반지', '동환율', '법인등기부등본', '해외선물', 'KODEX글로벌비만치료제TOP2PLUSETF', '콜드월렛', '한돈시세', '범일동맛집', '코인선물거래소', '베트남동', 'QQQM', 'AMD주가', '자동차보험조회', '코냑', '20대실비보험', '사이트', '다이렉트자동차보험료비교견적', '대학생공모전', '주식강의', '3월신용카드캐시백', 'ISA계좌이전', 'QQQ주가', '가지급금', '비트코인골드', '재택근무직업', '등기', '부동산등기', '데이터분석', '자동차보험다이렉트', '회계', '채권채무', '춘천대출', '24K금값', '현재금값', '퇴직금IRP', '근무일수계산기', '전세자금대출금리비교', '실비암보험', '매도', 'CI보험', '자동차보험료비교', 'AI주식투자', 'ELF', '바이넥스주가', '미국주식', '증권사', '삼성생명주가', '기업부설연구소', '3.3%계산기', '지분경매', '농협증권', '트레이딩뷰', '2월신용카드캐시백', '하나은행환율', '시바이누코인전망', '금한돈', '고시원', '파킹통장금리비교', '동물그림', '저작권료', '금전', '3월신용카드이벤트', '나스닥ETF', '카드사종류', '배당ETF', '수익률계산기', '미국배당주', '금리인하', '배당주추천', 'QLD주가', 'AI관련주', '생활비', 'DART', '공제보험', '소수점', '소액대출', '테더코인', '엔잡러', 'AI관련주식', '대여금', '비갱신형암보험', '3월공모주', 'ETF배당금', '아이큐코인', '엑셀가계부', '오늘상장주식', '오스코텍주가', '명함', '코인시세', '알테오젠주가', '펀딩사이트', 'S&P500ETF', '주식장', '해외선물하는법', '사직', '해외주식양도소득세', '코인지갑', 'STEP', '코리빙하우스', '테슬라주가', 'SCHD주가', '65세실비보험', '은퇴', '국세완납증명서발급', '연말정산환급금조회', '직계존속', '벤처기업인증', '토마토증권통', 'ELB', '비트코인거래소', '1억투자', '대출연체', '줍줍분양', '해외선물옵션', '퇴직금세금', '퇴직연금담보대출', 'HSCEI지수', '금시세', '비트코인관련주', '국내주식', '알트코인', 'IRP퇴직연금해지', '금1돈중량', '고배당ETF', '전고체관련주', '대출금계산기', '세제혜택', '메리츠', 'AI주식', '해외선물투자', '상속세신고기한', '대여금반환청구소송', '디지털트윈', '암보험', 'KB자동차보험', '최근상장주', '신용카드사', '지급명령강제집행', '에이다코인', '현금', '모텔임대', '인도증시', '평생교육원', '스테픈코인', '신용점수등급', '주식매매계약서', '만보기어플추천', 'IRP계좌개설', '차용증양식', '부동산매매', '프론트엔드', '상속등기서류', 'KODEX미국S&P500TR', '단기예금', '적립식투자', 'HLB주가', '비트코인선물', '대출상담', '상속한정승인', '소유권이전등기비용', '중개형ISA계좌', '배당주순위', '연말정산하는법', '프랜차이즈창업', 'KB라이프', '노후준비', '소득세계산법', '옵션거래', '비트코인선물거래', 'KODEXCD금리액티브(합성)', '디너의여왕체험단', '비트코인지갑', '금투자', 'NCS학습모듈', '시황', '전고체배터리관련주', '자차보험', '사직서작성방법', '코스피전망', '인도니프티50', '네이버주식토론방', '증여세신고', '비트코인전망', '밈코인', '조선주', '전세권설정등기', '특징주', '오늘금한돈가격', '티디엘상장', '슈퍼워크코인', 'CMA이자', '소상공인지원사업', '엔비디아주가', '엠블코인', '메리츠화재보험', '김프', '복비계산기', '설립', '신용정보', 'HBM관련주', 'VOO주가', '금주식', '간병인보험', '보험설계', '코인하는법', '세이코인', '홍콩H', 'SOLETF', '개인돈대출', '홍콩ELS', '인텔주가', '핸드폰부업', 'POLY', '광고사', '비상장주식거래소', '퇴직금지급기준', '자동차의무보험', '마멘토', '도지코인', '금가격', '사망보험', '비트버니', '네이버주가전망', '테슬라ETF', '해외선물거래', '금값전망', '정보공개', '상속전문변호사', '18K금시세', '추천주', '연말정산', '파이코인가격', '하나은행퇴직연금', 'WRAP', '입원보험', '홍콩H지수', '반도체ETF', '환율엔화', '손해보험', '전세대출금리비교', '사업자등록증', '오피스텔월세', '종합보험', '자동차다이렉트', '금값시세1돈', '금시세1돈', 'CD금리', '투자자산운용사', '집에서하는부업', '특허출원', '연말정산세액공제', '바이비트수수료', '자동차다이렉트보험비교', '암보험비갱신형', '채권추심', 'DEX거래소', '보험료조회', '공모전사이트', '코인거래', '미국배당ETF', '사업자금대출', '비트코인찾기', '청창사', '당뇨보험', '보험설계사', '차용증법적효력', '간병비보험', '순금', '자동차담보대출', '미국주식시장시간', 'INSURANCE', '증권사이벤트', '코인거래소', '복리예금', '자동차보험비교견적', '구리관련주', '가상자산', '단타매매', '알파벳주가', 'SCHDETF', '배당주투자', '코인거래소순위', '선물옵션', '공모전', '자동차보험료비교견적', '석상', '가압류절차', '부가세신고기간', '빗썸비트코인', '자동차보험료비교견적사이트', '리딩방', '강제집행', '3월카드캐시백', '배우자증여', 'PEF', '고액암종류', '한국코인거래소', '중고노트북매입', '상속포기신고서', '한미반도체주가', '환불', '3월상장주', '배당주', '법인등기', 'SOXX주가', '코인선물', 'PYTHON', '해외주식', '주식복리계산기', '재산명시', 'AIG손해보험', '적금추천', '상속인', '파이코인시세', '코인사이트', '정보공개청구', '동진쎄미켐주가', '지방세법', '대부업사업자등록', '일본주식', '유병자실비보험', '코인가격', '관세', '월세지원금', '수원금거래소', '건강보험다이렉트', '오늘의주식시황', '트레이딩', '나무증권', 'SCHD', '비트코인시세', '도지코인전망', '비트코인가격', '지급명령', '금한돈가격', '정치테마주', '인덱스펀드뜻', '실비보험추천', '실손보험가입', '블랙록', '일자리사이트', '대부업체조회', '대외활동', '법무사', '경험생명표', '14K금시세', 'IRP퇴직연금수령', '퇴직연금수령방법', '다이렉트보험', '분당전당포', '연말정산소득세', '레고켐바이오주가', '돈버는법', 'ISA계좌서민형', '오늘금값', '부가세', '씨씨에스주가', '비트코인차트', '솔라나시세', '중개수수료계산기', '건강보험종류', '퇴사사유', '소득세법', '지급명령신청', '명함제작', '미국채ETF', '자금', 'PI코인', '재산명시신청', '손오공주식', '대부대출', '신용카드발급', '부동산매물', '우미린아파트', '재택타이핑알바', '코인추천', '알바', '실비보험비교사이트', '차용', '일자리구하기', '전고체배터리대장주', '채권양도', '주주명부발급', '일자리', '보험종류', '미수금', '프랜차이즈박람회', '미국주식거래시간', '전세권설정', '와디즈상세페이지', '국내채권ETF', '상속포기신청', '사업자등록증발급', '캐피탈', '신규사업자대출', '청약일정', '자동차보험갱신', '자동차보험', '주택관리사', '테슬라주식', '선물거래소', '지원사업', '다이렉트자동차보험', 'KODEX미국반도체MV', '채권양도통지서', '대부업조회', 'TIGER미국S&P500', '중소기업지원사업', '순금시세', '앱', 'CMA통장', '데이마켓', '도지코인시세', '기대출과다자추가대출', '와이즈버즈', '성취', '비갱신실비보험', 'NH투자증권고객센터', '수채화그림', 'SOXL주가', 'ISC주가', '청년보험', '연말정산간소화', '대출금리계산기', '월세', '실비보험가입', '보험비교사이트', '연말정산기간', '일수계산', '주식용어', '미국채권ETF', '퇴직연금계좌', '비상금대출', '자동차보험비교견적사이트', '비트코인시세그래프', '대만환율', '금형제작', '조세특례제한법', '케이비손해보험', '4세대실비보험', '정부지원', '상속세세율', '코인판', 'ETF추천', '질권설정', 'OKX', '상속세계산기', '비트코인하는법', '원천징수', '재무제표', '경제신문', '배당기준일', '아파트월세', '정보공개법', '코인단타', '코인순위', '새마을금고예금', '상속세계산', '아파트', 'NH투자증권', 'KODEX테슬라인컴프리미엄채권혼합액티브', '코인차트', '60대실비보험', '코인거래방법', '차보험', '부가가치세법', '비갱신암보험', '오늘금시세', 'TSMC주가', '금1돈', '농협손해보험', '단기채권', '코오롱오두막', '9호선연장', '국내월배당ETF', '아파트취등록세계산기', '퇴사통보기간', '비만치료제관련주', '자동차보험가격비교', '타이거ETF', '하이닉스주가', '금1돈가격', 'TIGERETF', '저신용자대출', '무순위청약', '한화에어로스페이스주가', '주식차트사이트', '코인종류', 'AIG', '부가가치세', '울산다운2지구우미린모델하우스', '비트코인사는법', '목돈굴리기', '건강종합보험', '소득공제', '퇴직금', '주식계산기', '보험사', '메리츠금융지주', '연말정산환급금', '코인구매', '금ETF', '네이버주식', '요식업', '나스닥100', '카드이벤트', '주식사이트', '유아보험', '연말정산환급일', '주식투자', '일용직퇴직금', '전자지갑', '성년후견인신청', '어도비주가', 'KB', '청구이의의소', '법인차량보험', '모바일신용카드', 'KB다이렉트', '머니핀', '다모아보험', '대위변제', '엔비디아주식', 'E러닝', '한국비엔씨주가', '자동차보험비교', 'TIGER반도체', 'ISA계좌수수료', '1억모으기', '저점매수', '프리랜서대출', '월배당ETF', '금시세18K', '삼성전자주가전망', '한정승인', '상속세신고', '보험상담', '아파트청약', '근저당설정해지', '금1돈시세', '수술비보험', '법인자동차보험', '플레이댑', 'KODEX인도NIFTY50', 'NPL', '두나무주가']\n"
     ]
    }
   ],
   "source": [
    "async def collect_google_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"google_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            rising_keywords_results = pickle.load(file)\n",
    "    else:\n",
    "        rising_keywords_results = await collect_rising_keywords(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(rising_keywords_results, file)\n",
    "    \n",
    "    return rising_keywords_results\n",
    "\n",
    "async def collect_news_keywords(target_keywords):\n",
    "    today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "    directory_path = f\"./data/trend_data/{today_date}\"\n",
    "    file_path = os.path.join(directory_path, f\"news_data_{today_date}.pkl\")\n",
    "    \n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            news_data = pickle.load(file)\n",
    "    else:\n",
    "        news_data = await main_news(target_keywords)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(news_data, file)\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# 메인 비동기 실행 함수\n",
    "async def main(target_keywords):\n",
    "    google_keywords_results, news_keywords_results = await asyncio.gather(\n",
    "        collect_google_keywords(target_keywords),\n",
    "        collect_news_keywords(target_keywords)\n",
    "    )\n",
    "    \n",
    "    return google_keywords_results, news_keywords_results\n",
    "target_keywords = list(set(info_result_final['연관키워드']))\n",
    "print(target_keywords)\n",
    "\n",
    "rising_keywords_results,news_data=asyncio.run(main(target_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/target_keywords/240322\\target_keywords.txt 파일이 이미 존재합니다. 작업을 건너뜁니다.\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "#활동성 분석\n",
    "################################\n",
    "\n",
    "today_date = datetime.now().strftime(\"%y%m%d\")\n",
    "directory_path = f\"./data/target_keywords/{today_date}\"\n",
    "file_path = os.path.join(directory_path, \"target_keywords.txt\")\n",
    "\n",
    "if not os.path.exists(directory_path):\n",
    "    # 디렉토리가 존재하지 않는 경우, 디렉토리 생성\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# 파일이 존재하는지 확인\n",
    "if not os.path.exists(file_path):\n",
    "    # 키워드를 파일에 작성\n",
    "    with open(file_path, 'w') as file:\n",
    "        for keyword in target_keywords:\n",
    "            file.write(\"%s\\n\" % keyword)\n",
    "    result = f\"{file_path}에 키워드 저장됨\"\n",
    "else:\n",
    "    result = f\"{file_path} 파일이 이미 존재합니다. 작업을 건너뜁니다.\"\n",
    "\n",
    "print(result)\n",
    "file_path = './data/target_keywords/240322/keyword_activity_rates.csv'\n",
    "if os.path.exists(file_path):\n",
    "    pass  # 파일이 존재하면 여기에서 처리를 건너뜁니다.\n",
    "else:\n",
    "    # 파일이 존재하지 않는 경우, activity_rate 함수를 호출\n",
    "    activity_rate(f'{directory_path}/target_keywords.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# 뉴스링크,제목,연관검색어 데이터프레임 생성\n",
    "#######################################\n",
    "\n",
    "\n",
    "name_list = list(news_data.keys())  \n",
    "# DataFrame 초기화\n",
    "news_df = pd.DataFrame()\n",
    "\n",
    "# 모든 키워드에 대해 처리\n",
    "for keyword in name_list:\n",
    "    # 뉴스 항목이 있는 경우 데이터 추가\n",
    "    for news_item in news_data[keyword]:\n",
    "        news_row = [keyword, news_item[0], news_item[1]]  # 연관키워드, 뉴스제목, 뉴스링크\n",
    "        news_df = pd.concat([news_df, pd.DataFrame([news_row])], ignore_index=True)\n",
    "\n",
    "    # 뉴스 항목 수가 10개에 미치지 못하면 나머지를 빈 행으로 채움\n",
    "    for _ in range(10 - len(news_data[keyword])):\n",
    "        empty_row = [keyword, None, None]  # 연관키워드, 빈 뉴스제목, 빈 뉴스링크\n",
    "        news_df = pd.concat([news_df, pd.DataFrame([empty_row])], ignore_index=True)\n",
    "\n",
    "# 칼럼 이름 설정\n",
    "news_df.columns = ['연관검색어', '뉴스제목', '뉴스링크']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_activity_rates = pd.read_csv(f'{directory_path}/keyword_activity_rates.csv')\n",
    "keyword_activity_rates.columns = ['연관검색어', '활동성']\n",
    "\n",
    "# '활동성' 열의 데이터를 백분율 형태의 문자열로 변환\n",
    "keyword_activity_rates['활동성'] = keyword_activity_rates['활동성'].apply(lambda x: f\"{x}%\")\n",
    "# news_df와 keyword_activity_rates를 '연관검색어' 열을 기준으로 병합\n",
    "keyword_activity_rates = keyword_activity_rates.drop_duplicates(subset=['연관검색어'])\n",
    "merged_keyword_activity_rates = pd.merge(news_df, keyword_activity_rates, on='연관검색어', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "# 네이버 merge\n",
    "####\n",
    "collected_keywords_dat_copy.rename(columns={'연관키워드': '연관검색어'}, inplace=True)\n",
    "info_result_final.rename(columns={'연관키워드': '연관검색어'}, inplace=True)\n",
    "# collected_keywords_dat_copy에서 '연관키워드'와 '검색어'를 기준으로 중복 제거\n",
    "collected_keywords_dat_copy = collected_keywords_dat_copy.drop_duplicates(subset=['연관검색어'], keep='first')\n",
    "# 이제 merged_keyword_activity_rates와 결합\n",
    "final_merged_df = pd.merge(merged_keyword_activity_rates, collected_keywords_dat_copy[['연관검색어', '검색어']], on='연관검색어', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_58764\\1304442105.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_keywords_by_search = collected_keywords_dat_copy.groupby('검색어').apply(\n"
     ]
    }
   ],
   "source": [
    "final_merged_df_copy = final_merged_df.copy()\n",
    "\n",
    "# 구글검색어 컬럼을 초기화합니다.\n",
    "final_merged_df_copy['구글검색어'] = None\n",
    "\n",
    "# 이후의 모든 작업은 final_merged_df_copy에 대해 수행합니다.\n",
    "i = 0\n",
    "for keyword, queries in rising_keywords_results.items():\n",
    "    filled_queries = queries[:10] + [None] * (10 - len(queries[:10]))\n",
    "    for query in filled_queries:\n",
    "        if i < len(final_merged_df_copy):\n",
    "            final_merged_df_copy.at[i, '구글검색어'] = query\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "# final_merged_df의 '검색어' 컬럼에서 각 10번째 검색어를 추출합니다.\n",
    "keyword_list_per_10 = final_merged_df_copy['검색어'].tolist()[::10]\n",
    "\n",
    "\n",
    " \n",
    "# collected_keywords_dat_copy에서 각 검색어별 상위 10개 연관검색어를 가져옵니다.\n",
    "# 여기서는 각 검색어별로 가장 높은 월간검색수를 가진 상위 10개를 선정합니다.\n",
    "top_keywords_by_search = collected_keywords_dat_copy.groupby('검색어').apply(\n",
    "    lambda x: x.nlargest(10, '월간검색수_합계')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# 새로운 DataFrame을 초기화합니다. 이 DataFrame에는 각 검색어별 상위 10개 연관검색어가 포함됩니다.\n",
    "new_rows_for_final_df = []\n",
    "\n",
    "\n",
    "for keyword in keyword_list_per_10:\n",
    "    # 특정 키워드에 대한 상위 10개 연관 검색어 추출\n",
    "    top_queries_for_keyword = top_keywords_by_search[top_keywords_by_search['검색어'] == keyword].head(10)\n",
    "    \n",
    "    # 추출된 연관 검색어를 결과 리스트에 추가\n",
    "    num_rows_added = 0  # 추가된 연관 검색어의 수를 추적\n",
    "    for _, row in top_queries_for_keyword.iterrows():\n",
    "        new_rows_for_final_df.append(row['연관검색어'])\n",
    "        num_rows_added += 1\n",
    "    \n",
    "    # 10개 미만인 경우 나머지를 None으로 채우기\n",
    "    for _ in range(10 - num_rows_added):\n",
    "        new_rows_for_final_df.append(None)\n",
    "\n",
    "\n",
    "# new_rows_for_final_df의 길이를 확인하고 final_merged_df의 '네이버검색어' 컬럼에 값을 할당합니다.\n",
    "# 주의: new_rows_for_final_df의 길이가 final_merged_df의 행 수와 동일해야 합니다.\n",
    "# 만약 길이가 다르다면, 길이가 맞도록 조정이 필요합니다.\n",
    "if len(new_rows_for_final_df) == len(final_merged_df_copy):\n",
    "    final_merged_df_copy['네이버검색어'] = new_rows_for_final_df\n",
    "else:\n",
    "    print(\"경고: '네이버검색어' 데이터의 길이가 final_merged_df와 다릅니다. 데이터 확인이 필요합니다.\")\n",
    "\n",
    "# 최종 DataFrame 확인\n",
    "#final_merged_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형식 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_58764\\2013373558.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_58764\\2013373558.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['연관검색어'] = info_result_af_copy_reordered['연관검색어'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_58764\\2013373558.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['네이버검색어'] = info_result_af_copy_reordered['네이버검색어'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_58764\\2013373558.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['구글검색어'] = info_result_af_copy_reordered['구글검색어'].str.replace(\"|\", \"\")\n",
      "C:\\Users\\chohy\\AppData\\Local\\Temp\\ipykernel_58764\\2013373558.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"–\", \"-\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#info_result_final = info_result_final.drop(columns=[\"일별 급상승\", \"주별 급상승\", \"주별 지속상승\", \"월별 급상승\", \"월별 지속상승\", \"월별 규칙성\"])\n",
    "\n",
    "final_merged_df_result = pd.merge(info_result_final, final_merged_df_copy, how='left', on='연관검색어')\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# '기준일자' 컬럼을 가장 앞에 추가\n",
    "final_merged_df_result.insert(0, '기준일자', today_date)\n",
    "# 컬럼명 변경: '중복검색어' -> '검색키워드', '월간검색수_합계' -> '검색량'\n",
    "\n",
    "final_merged_df_result.rename(columns={'중복검색어': '검색키워드', '월간검색수_합계': '검색량'}, inplace=True)\n",
    "\n",
    "final_merged_df_result = final_merged_df_result.drop(columns=[\"검색어\"])\n",
    "\n",
    "\n",
    "final_merged_df_result['상승월'] = None\n",
    "# rising_month_list의 각 항목에 대해 반복 처리\n",
    "for month_info in rising_month_list:\n",
    "    months, keyword = month_info  # month_info는 각각의 월 목록과 키워드를 포함합니다.\n",
    "    keyword_rows = final_merged_df_result[final_merged_df_result['연관검색어'] == keyword]  # 해당 키워드에 대한 행만 선택합니다.\n",
    "    \n",
    "    if not keyword_rows.empty:\n",
    "\n",
    "        for i, month in enumerate(months):\n",
    "            if i < len(keyword_rows):\n",
    "                final_merged_df_result.loc[keyword_rows.index[i], '상승월'] = month\n",
    "            else:\n",
    "                break  # 월의 개수보다 더 많은 행에 대해서는 처리를 중단합니다.\n",
    "\n",
    "\n",
    "\n",
    "# 형식맞추기 위한 info_result_final 순서 정렬\n",
    "info_result_af_copy=pd.DataFrame()\n",
    "a = final_merged_df_result.query(\"`유형` == '일별 급상승'\")\n",
    "b = final_merged_df_result.query(\"`유형` == '주별 급상승' or `유형` == '주별 지속상승'\")\n",
    "c = final_merged_df_result.query(\"`유형` == '월별 급상승' or `유형` == '월별 지속상승' or `유형` == '월별 규칙성'\")\n",
    "a_sort=a.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "b_sort = b.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "c_sort = c.sort_values(by=['연관검색어', '유형'], ascending=[True, True])\n",
    "info_result_af_copy=pd.concat([a_sort,b_sort,c_sort])\n",
    "\n",
    "# 형식을 위한 이름 변경\n",
    "new_column_order = ['기준일자', '유형', '연관검색어', '검색키워드', '검색량', '지표', '뉴스제목', '뉴스링크', '활동성', '구글검색어', '네이버검색어', '상승월']\n",
    "info_result_af_copy_reordered = info_result_af_copy[new_column_order]\n",
    "\n",
    "# 혹시나 모를 예외처리(형식에 어긋나는 것을 예방)\n",
    "info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['연관검색어'] = info_result_af_copy_reordered['연관검색어'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['네이버검색어'] = info_result_af_copy_reordered['네이버검색어'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['구글검색어'] = info_result_af_copy_reordered['구글검색어'].str.replace(\"|\", \"\")\n",
    "info_result_af_copy_reordered['뉴스제목'] = info_result_af_copy_reordered['뉴스제목'].str.replace(\"–\", \"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유형 순서 정렬\n",
    "info_result_af_copy_reordered_modified = info_result_af_copy_reordered.copy()\n",
    "\n",
    "\n",
    "# 인덱스 재설정\n",
    "info_result_af_copy_reordered_modified.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sort_order = {\n",
    "    \"일별 급상승\": 1,\n",
    "    \"주별 급상승\": 2,\n",
    "    \"주별 지속상승\": 3,\n",
    "    \"월별 급상승\": 4,\n",
    "    \"월별 지속상승\": 5,\n",
    "    \"월별 규칙성\" : 6\n",
    "}\n",
    "\n",
    "# 유형 컬럼에 대한 정렬 순서를 적용하기 위해 임시 컬럼 추가\n",
    "info_result_af_copy_reordered_modified['sort_key'] = info_result_af_copy_reordered_modified['유형'].map(sort_order)\n",
    "\n",
    "# 임시 컬럼을 기준으로 정렬\n",
    "info_result_af_copy_reordered_modified = info_result_af_copy_reordered_modified.sort_values(by=['sort_key', '연관검색어'], ascending=[True, True])\n",
    "\n",
    "# 임시 컬럼 삭제\n",
    "info_result_af_copy_reordered_modified.drop('sort_key', axis=1, inplace=True)\n",
    "\n",
    "info_result_af_copy_reordered_modified.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형식에 맞춰서 띄어쓰기 변경\n",
    "# '유형' 컬럼의 값을 바꾸기 위한 딕셔너리 정의\n",
    "replace_values = {\n",
    "    '일별 급상승': '일별급상승',\n",
    "    '주별 급상승': '주별급상승',\n",
    "    '주별 지속상승': '주별지속상승',\n",
    "    '월별 급상승': '월별급상승',\n",
    "    '월별 지속상승': '월별지속상승',\n",
    "    '월별 규칙성': '월별규칙성'\n",
    "}\n",
    "\n",
    "# '유형' 컬럼 내의 값을 바꾸기\n",
    "graph_result['유형'] = graph_result['유형'].replace(replace_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_related_search_terms = list(graph_result[pd.isna(graph_result['검색량'])]['연관검색어'])\n",
    "unique_na_related_search_terms  = list(set(na_related_search_terms))\n",
    "\n",
    "filtered_graph_result = graph_result[~graph_result['연관검색어'].isin(unique_na_related_search_terms)]\n",
    "\n",
    "\n",
    "filtered_info_result_af_copy_reordered_modified = info_result_af_copy_reordered_modified[~info_result_af_copy_reordered_modified['연관검색어'].isin(unique_na_related_search_terms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_graph_result_updated = filtered_graph_result.iloc[:, :-2]\n",
    "filtered_graph_result_updated_a = filtered_graph_result_updated[filtered_graph_result_updated['유형'] == '일별급상승']\n",
    "\n",
    "# 올바른 조건을 사용하여 필터링\n",
    "filtered_graph_result_updated_b = filtered_graph_result_updated[\n",
    "    filtered_graph_result_updated['유형'].isin(['월별급상승', '월별지속상승', '월별규칙성'])]\n",
    "filtered_graph_result_updated_c = filtered_graph_result_updated[\n",
    "    filtered_graph_result_updated['유형'].isin(['주별급상승', '주별지속상승'])]\n",
    "sorted_filtered_graph_result_updated_b = filtered_graph_result_updated_b.sort_values(by=['연관검색어', '유형', '검색일자'])\n",
    "sorted_filtered_graph_result_updated_c = filtered_graph_result_updated_c.sort_values(by=['연관검색어', '유형', '검색일자'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([filtered_graph_result_updated_a, sorted_filtered_graph_result_updated_c, sorted_filtered_graph_result_updated_b], axis=0)\n",
    "combined_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전송용 결과 테이블 생성 함수\n",
    "\n",
    "def make_csv(table) :\n",
    "\n",
    "  # 컬럼 추출\n",
    "  col_a = ''\n",
    "  col_b = ''\n",
    "\n",
    "  for col in table.columns :\n",
    "    col_a = str(col) + '|||'\n",
    "    col_b = col_b + col_a\n",
    "  col_b\n",
    "\n",
    "\n",
    "  # 행 추출\n",
    "  row_list = []\n",
    "\n",
    "  for j in range(0, len(table)) :\n",
    "    tmp_a = ''\n",
    "    tmp_b = ''\n",
    "\n",
    "    for i in range(0, len(table.columns)) :\n",
    "      tmp_a = str(table.iloc[j,i]) + '|||'\n",
    "      tmp_b = tmp_b + tmp_a\n",
    "    row_list.append(tmp_b)\n",
    "\n",
    "  row_list.insert(0,col_b)\n",
    "  df = pd.DataFrame(row_list)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data = filtered_info_result_af_copy_reordered_modified.fillna(' ')\n",
    "info_data.reset_index(inplace = True, drop = True)\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = make_csv(info_data)\n",
    "\n",
    "# 현재 날짜를 'yyMMdd' 형식으로 포맷팅\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n",
    "\n",
    "# 저장할 경로\n",
    "save_path = f'./data/result_out/{formatted_today}'\n",
    "\n",
    "# 해당 경로가 존재하지 않으면 생성\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# CSV 파일 저장\n",
    "result_csv.to_csv(f'{save_path}/info_{formatted_today}.csv', encoding='utf-8-sig', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_a = combined_df[combined_df['유형'] == '일별급상승']\n",
    "\n",
    "# 올바른 조건을 사용하여 필터링\n",
    "combined_df_b = combined_df[\n",
    "    combined_df['유형'].isin(['월별급상승', '월별지속상승', '월별규칙성'])]\n",
    "combined_df_c = combined_df[\n",
    "    combined_df['유형'].isin(['주별급상승', '주별지속상승'])]\n",
    "sorted_combined_df__b = combined_df_b.sort_values(by=['유형', '연관검색어', '검색일자'])\n",
    "sorted_combined_df__c = combined_df_c.sort_values(by=['유형', '연관검색어', '검색일자'])\n",
    "combined_df = pd.concat([combined_df_a, sorted_combined_df__c, sorted_combined_df__b], axis=0)\n",
    "combined_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_graph = make_csv(combined_df)\n",
    "\n",
    "\n",
    "# 현재 날짜를 'yyMMdd' 형식으로 포맷팅\n",
    "today = datetime.now(timezone('Asia/Seoul'))\n",
    "formatted_today = today.strftime('%y%m%d')\n",
    "\n",
    "# 저장할 경로\n",
    "save_path = f'./data/result_out/{formatted_today}'\n",
    "\n",
    "# 해당 경로가 존재하지 않으면 생성\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# CSV 파일 저장\n",
    "result_graph.to_csv(f'{save_path}/graph_{formatted_today}.csv', encoding='utf-8-sig', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
